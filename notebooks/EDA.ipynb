{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "Load datasets from `../data` and preview.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e9b860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = Path('..') / 'data'\n",
    "dataframes = {}\n",
    "\n",
    "for path in sorted(data_dir.iterdir()):\n",
    "    if path.suffix.lower() == '.csv':\n",
    "        df = pd.read_csv(path)\n",
    "        dataframes[path.stem] = df\n",
    "    elif path.suffix.lower() == '.npy':\n",
    "        arr = np.load(path, allow_pickle=True)\n",
    "        if arr.ndim == 1:\n",
    "            df = pd.DataFrame(arr, columns=[path.stem])\n",
    "        else:\n",
    "            df = pd.DataFrame(arr)\n",
    "        dataframes[path.stem] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aacae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    print(f'=== {name} | shape: {df.shape} ===')\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a721bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Interaction Features: Negative + Click + Like + Comment + Gift\n",
    "#### 1.1 Click\n",
    "##### 1.1.1 Rename timestamp to imp_timestamp, Convert it to datetime feature \n",
    "##### 1.1.2  Derive is_click --> df_click\n",
    "\n",
    "#### 1.2 like\n",
    "##### 1.2.1 Rename timestamp to like_timestamp, Convert it to datetime feature\n",
    "##### 1.2.2 Derive is_like\n",
    "##### 1.2.3 Merge like -> latest prior click per user_id/live_id/streamer_id --> df_click_with_like\n",
    "\n",
    "\n",
    "#### 1.3 comment\n",
    "##### 1.3.1 Rename timestamp to like_timestamp, Convert it to datetime feature \n",
    "##### 1.3.2 Derive is_comment\n",
    "##### 1.3.3 Merge comment data -> latest prior click per user_id/live_id/streamer_id --> df_click_with_like_comment\n",
    "\n",
    "#### 1.4 gift\n",
    "##### 1.4.1 Rename timestamp to gift_timestamp, Convert it to datetime feature \n",
    "##### 1.4.2 Derive is_gift\n",
    "##### 1.4.3 Merge gift data -> latest prior click per user_id/live_id/streamer_id --> df_click_with_like_comment_gift\n",
    "\n",
    "#### 1.5 Negative\n",
    "##### 1.5.1 Rename timestamp to imp_timestamp, Convert it to datetime feature \n",
    "##### 1.5.2 Derive is_click, watch_live_time, is_like, is_comment, is_gift, gift_price and set their values all as 0 \n",
    "##### 1.5.3 Derive latest_like_ts, latest_comment_ts, latest_gift_ts and set as 0\n",
    "##### 1.5.4 Vertically append with df_click_with_like_comment_gift to generate the final dataset \"df_interactions\"\n",
    "##### 1.5.5 Sort df_interactions by imp_timestamp, user_id, live_id, streamer_id ---> df_interactions\n",
    "##### 1.5.6 Derive imp_year/imp_month/imp_day/imp_hour/imp_is_weekend and reorder columns\n",
    "\n",
    "### 2. User Features\n",
    "#### 2.1 Convert reg_timestamp and first_watch_live_timestamp to datetime features\n",
    "#### 2.2 rename features \n",
    "\t\t# 1) age -->  user_age\n",
    "\t\t# 2) gender --> user_gender\n",
    "\t\t# 3) country --> user_country\n",
    "\t\t# 4) device_brand --> user_device_brand\n",
    "\t\t# 5) device_price --> user_device_price\n",
    "\t\t# 6) reg_timestamp --> user_reg_timestamp\n",
    "        # 7) onehot_feat0 --- onehot_feat6 --> user_onehot_feat0 --- user_onehot_feat6\n",
    "#### 2.3 Label encoding\n",
    "\t\t# 1) user_age --> user_age_le\n",
    "\t\t# 2) user_gener --> user_gender_le\n",
    "\t\t# 3) country --> user_country_le\n",
    "\t\t# 4) device_brand --> user_device_brand_le\n",
    "\t\t# 5) device_price --> user_device_price_le\n",
    "\t\t# 6) fans_num --> fans_num_le\n",
    "        # 7) follow_num --> follow_num_le\n",
    "\t\t# 8) accu_watch_live_cnt --> accu_watch_live_cnt_le\n",
    "\t\t# 9) accu_watch_live_duration --> accu_watch_live_duration_le\n",
    "\n",
    "### 3. Room features\n",
    "#### 3.1 Convert p_date, start_timestamp and end_timestamp to datetime features\n",
    "#### 3.2 Label encoding \n",
    "\t\t# live_content_category --> see unique values and missingness ---> label encoding --> live_content_category_le\n",
    "#### 3.3 New features \n",
    "\t\t#xxx 1) time_since_live_sart (ms) = imp_timestamp - start_timestamp (Pending after merge with interaction)\n",
    "\t\t# 2) live_start_year\n",
    "\t\t# 3) live_start_month\n",
    "\t\t# 4) live_start_day\n",
    "\t\t# 5) live_start_hour\n",
    "\t\t# 6) live_is_weekend\n",
    "#### 3.4 merge title_embedding dataset by live_name_id\n",
    "#### 3.5 Fill missing embedding with 0 and derive a flag variable title_emb_missing\n",
    "#### 3.6 check missing of all variables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 4. streamer features\n",
    "#### 4.1 Convert reg_timestamp and first_live_timestamp to datetime features\n",
    "#### 4.2 Rename\n",
    "\t\t# 1) age -->  streamer_age\n",
    "\t\t# 2) gender --> streamer_gender\n",
    "\t\t# 3) country --> streamer_country\n",
    "\t\t# 4) device_brand --> streamer_divice_brand\n",
    "\t\t# 5) device_price --> streamer_device_price\n",
    "\t\t# 6) reg_timestamp --> streamer_reg_timestamp\n",
    "\t\t# 7) onehot_feat0 --- onehot_feat6 --> streamer_onehot_feat0 --- streamer_onehot_feat6\n",
    "#### 4.3 Label Encoding\n",
    "\t\t# 1) streamer_age --> see unique values and missingness ---> label encoding --> streamer_age_le\n",
    "\t\t# 2) streamer_gender --> see unique values and missingness ---> label encoding --> streamer_gender_le (pending)\n",
    "\t\t# 3) streamer_country --> see unique values and missingness ---> label encoding --> streamer_country_le\n",
    "\t\t# 4) streamer_device_brand --> see unique values and missingness ---> streamer_device_brand_le\n",
    "\t\t# 5) streamer_device_price --> see unique values and missingness ---> streamer_device_price_le\n",
    "\t\t# 6) live_operation_tag --> see unique values and missingness ---> live_operation_tag_le\n",
    "\t\t# 7) fans_user_num --> see unique values and missingness ---> fans_user_num_le\n",
    "\t\t# 8) fans_group_fans_num --> see unique values and missingness ---> fans_group_fans_num_le\n",
    "\t\t# 9) follow_user_num --> see unique values and missingness ---> follow_user_num_le\n",
    "\t\t# 10) accu_live_cnt --> see unique values and missingness --->  accu_live_cnt_le\n",
    "\t\t# 11) accu_live_duration --> see unique values and missingness --->  accu_live_duration_le\n",
    "\t\t# 12) accu_play_cnt --> see unique values and missingness --->  accu_play_cnt_le\n",
    "\t\t# 13) accu_play_duration --> see unique values and missingness --->  accu_play_duration_le\n",
    "#### 4.4 check missing of all variables\n",
    "\n",
    "\n",
    "\n",
    "### 5. concatenation\n",
    "#### 5.1 merge df_room + df_streamer by streamer_id --> df_room_streamer \n",
    "#### 5.2 merge df_interactions + df_user by user_id--> df_interaction_user (47?)\n",
    "#### 5.3 merge df_interaction_user + df_room_streamer by streamer_id and live_id (223) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 6. Other derivation (Contextual / Temporal / Cross Features)\n",
    "#### 6.1 User Features\n",
    "##### 6.1.1 Basic\n",
    "\t\t\t# 1) user_account_age = imp_timestamp - user_reg_timestamp\n",
    "\t\t\t# 2) user_watch_live_age = imp_timestamp - first_watch_live_timestamp\n",
    "\n",
    "##### 6.1.2 User CTR (pre-impression, denominator: impressions)\n",
    "\t\t\t# ctr_user_15min\n",
    "\t\t\t# ctr_user_3hr\n",
    "\t\t\t# ctr_user_1d\n",
    "\t\t\t# ctr_user_7d\n",
    "##### 6.1.3 User exposure fatigue - Imp\n",
    "\t\t\t# num_imp_user_10min\n",
    "\t\t\t# num_imp_user_30min\n",
    "\t\t\t# num_imp_user_2hr\n",
    "\t\t\t# num_imp_user_12hr\n",
    "\t\t\t# num_imp_user_1d\n",
    "\t\t\t# num_imp_user_7d\n",
    "##### 6.1.4 User click fatigue - click\n",
    "\t\t\t# num_click_user_15min\n",
    "\t\t\t# num_click_user_3hr\n",
    "\t\t\t# num_click_user_1d\n",
    "\t\t\t# num_click_user_7d\n",
    "\t\t\t# click_trend_user = log(num_click_user_15min + 1) - log(num_click_user_3hr + 1)\n",
    "##### 6.1.5 User recency\n",
    "\t\t\t# time_since_last_impression_user\n",
    "\t\t\t# tsli_missing - 1 if the user has no prior impression (first impression); 0 - otherwise\n",
    "\t\t\t# time_since_last_click_user\n",
    "\t\t\t# tslc_missing - - 1 if the user has no reliable prior click (never clicked or click masked by causality guard); 0 - otherwise\n",
    "\t\t\t# consecutive_skips_user: number of impressions since last click\n",
    "##### 6.1.6 User dwell / engagement quality (from past clicks only)\n",
    "\t\t\t# avg_watch_time_user\n",
    "\t\t\t# avg_watch_time_user\n",
    "\t\t\t# median_watch_time_user\n",
    "\t\t\t# median_watch_time_user\n",
    "\t\t\t# pct_long_watch_user_30s\n",
    "##### 6.1.7 User comment behavior (Denominator: clicks)\n",
    "\t\t\t# comment_rate_user = (num_comment_user + 1) / (num_click_user + 1)\n",
    "\t\t\t# has_comment_user_24h\n",
    "\t\t\t# num_comment_user_24h\n",
    "##### 6.1.8 User like behavior (Denominator: clicks)\n",
    "\t\t\t# like_rate_user = (num_like_user + 1) / (num_click_user + 1)\n",
    "\t\t\t# has_like_user_24h\n",
    "\t\t\t# num_like_user_24h\n",
    "##### 6.1.9 User gift behavior\n",
    "\t\t\t# has_gift_user_7d\n",
    "\t\t\t# num_gift_user_7d\n",
    "\t\t\t# amount_gift_user_7d\n",
    "\n",
    "\n",
    "#### 6.2 Room (Live) Features\n",
    "##### 6.2.1 Basic\n",
    "\t\t\t# time_since_live_start (ms) = imp_timestamp - start_timestamp\n",
    "##### 6.2.2 Room CTR (pre-impression, denominator: impressions)\n",
    "\t\t\t# ctr_room_10min\n",
    "\t\t\t# ctr_room_30min\n",
    "\t\t\t# ctr_room_2hr\n",
    "\t\t\t# ctr_room_12hr\n",
    "##### 6.2.3 Room exposure volume - imp\n",
    "\t\t\t# num_imp_room_10min\n",
    "\t\t\t# num_imp_room_30min\n",
    "\t\t\t# num_imp_room_2hr\n",
    "\t\t\t# num_imp_room_12hr\n",
    "\t\t\t# num_imp_room_1d\n",
    "##### 6.2.4 Room click volume - click\n",
    "\t\t\t# num_click_room_10min\n",
    "\t\t\t# num_click_room_30min\n",
    "\t\t\t# num_click_room_2hr\n",
    "\t\t\t# num_click_room_12hr\n",
    "\t\t\t# num_click_room_1d\n",
    "\t\t\t# ctr_trend_room = log(ctr_room_10min + 1e-6) - log(ctr_room_2hr + 1e-6)\n",
    "##### 6.2.5 Room freshness (leakage-safe)\n",
    "\t\t\t# time_since_start_live\n",
    "\t\t\t# time_since_start_live_bucket: (<5min, 5-20min, >20min)\n",
    "##### 6.2.6 Room dwell / engagement quality (from past clicks only)\n",
    "\t\t\t# avg_watch_time_live\n",
    "\t\t\t# median_watch_time_live\n",
    "\t\t\t# watch_time_live_missing\n",
    "\t\t\t# avg_watch_time_live_30min\n",
    "\t\t\t# median_watch_time_live_30min\n",
    "\t\t\t# watch_time_live_30min_missing\n",
    "\t\t\t# pct_long_watch_live_60s_30min\n",
    "##### 6.2.7 Room comment behavior (Denominator: impressions)\n",
    "\t\t\t# comment_rate_live\n",
    "\t\t\t# comment_rate_live_15min\n",
    "\t\t\t# comment_rate_live_1hr\n",
    "\t\t\t# comment_rate_live_3hr\n",
    "\t\t\t# num_comment_live\n",
    "\t\t\t# num_comment_live_15min\n",
    "\t\t\t# num_comment_live_1hr\n",
    "\t\t\t# num_comment_live_3hr\n",
    "\t\t\t# comment_trend_room = log(comment_rate_live_15min + 1e-6) - log(comment_rate_live_1hr + 1e-6)\n",
    "##### 6.2.8 Room like behavior (Denominator: impressions)\n",
    "\t\t\t# like_rate_live\n",
    "\t\t\t# like_rate_live_15min\n",
    "\t\t\t# like_rate_live_1hr\n",
    "\t\t\t# like_rate_live_3hr\n",
    "\t\t\t# num_like_live\n",
    "\t\t\t# num_like_live_15min\n",
    "\t\t\t# num_like_live_1hr\n",
    "\t\t\t# num_like_live_3hr\n",
    "\t\t\t# like_trend_room = log(like_rate_live_15min + 1e-6) - log(like_rate_live_1hr + 1e-6)\n",
    "##### 6.2.9 Room gift behavior (Denominator: impressions)\n",
    "\t\t\t# gift_rate_live\n",
    "\t\t\t# gift_rate_live_15min\n",
    "\t\t\t# gift_rate_live_1hr\n",
    "\t\t\t# gift_rate_live_3hr\n",
    "\t\t\t# num_gift_live\n",
    "\t\t\t# num_gift_live_15min\n",
    "\t\t\t# num_gift_live_1hr\n",
    "\t\t\t# num_gift_live_3hr\n",
    "\t\t\t# amount_gift_live\n",
    "\t\t\t# amount_gift_live_15min\n",
    "\t\t\t# amount_gift_live_1hr\n",
    "\t\t\t# amount_gift_live_3hr\n",
    "\t\t\t# gift_trend_room = log(log_amount_gift_room_15min + 1) - log(log_amount_gift_room_1hr + 1)\n",
    "\n",
    "\n",
    "#### 6.3 Streamer Features\n",
    "##### 6.3.1 Basic\n",
    "\t\t\t# streamer_account_age = imp_timestamp - streamer_reg_timestamp\n",
    "\t\t\t# streamer_live_age = imp_timestamp - first_live_timestamp\n",
    "##### 6.3.2 Streamer CTR / volume\n",
    "\t\t\t# ctr_streamer_1d\n",
    "\t\t\t# ctr_streamer_7d\n",
    "\t\t\t# num_imp_streamer_7d\n",
    "\t\t\t# num_click_streamer_7d\n",
    "\t\t\t# num_lives_streamer_7d\n",
    "##### 6.3.3 Streamer engagement quality - dwell time (from past clicks only)\n",
    "\t\t\t# avg_watch_time_streamer\n",
    "\t\t\t# median_watch_time_streamer\n",
    "\t\t\t# pct_long_watch_streamer_30s\n",
    "\t\t\t# watch_time_streamer_missing\n",
    "\n",
    "##### 6.3.4 Streamer interaction volume\n",
    "\t\t\t# num_comment_streamer_7d\n",
    "\t\t\t# num_like_streamer_7d\n",
    "\t\t\t# amount_gift_streamer_7d\n",
    "\n",
    "\n",
    "#### 6.4 Cross Features (High ROI)\n",
    "##### 6.4.1 User x streamer\n",
    "\t\t\t# ctr_user_streamer_7d\n",
    "\t\t\t# num_click_user_streamer_7d\n",
    "\t\t\t# num_imp_user_streamer_7d\n",
    "\t\t\t# time_since_last_impression_user_streamer\n",
    "\t\t\t# time_since_last_click_user_streamer\n",
    "##### 6.4.2 User x category\n",
    "\t\t\t# ctr_user_category_7d\n",
    "\t\t\t# num_click_user_category_7d\n",
    "\t\t\t# num_imp_user_category_7d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97208c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1.1 Click\n",
    "##### 1.1.1 Rename timestamp to imp_timestamp, Convert it to datetime feature \n",
    "##### 1.1.2  Derive is_click\n",
    "df_click = dataframes.get(\"click\")\n",
    "\n",
    "if df_click is not None:\n",
    "    df_click = df_click.copy()\n",
    "\n",
    "    if \"timestamp\" in df_click.columns:\n",
    "        df_click = df_click.rename(columns={\"timestamp\": \"imp_timestamp\"})\n",
    "        df_click[\"imp_timestamp\"] = pd.to_datetime(df_click[\"imp_timestamp\"], unit=\"ms\", errors=\"coerce\")\n",
    "\n",
    "    df_click[\"is_click\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e70020",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_click.head())\n",
    "print(df_click.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b35f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1.2 like\n",
    "##### 1.2.1 Rename timestamp to imp_timestamp, Convert it to datetime feature Derive is_click\n",
    "##### 1.2.2  Derive is_click\n",
    "df_like = dataframes.get(\"like\")\n",
    "\n",
    "if df_like is not None:\n",
    "    df_like = df_like.copy()\n",
    "\n",
    "    if \"timestamp\" in df_like.columns:\n",
    "        df_like = df_like.rename(columns={\"timestamp\": \"like_timestamp\"})\n",
    "        df_like[\"like_timestamp\"] = pd.to_datetime(df_like[\"like_timestamp\"], unit=\"ms\", errors=\"coerce\")\n",
    "\n",
    "    df_like[\"is_like\"] = 1\n",
    "\n",
    "display(df_like.head())\n",
    "print(df_like.shape)\n",
    "\n",
    "##### 1.2.3 Merge like -> latest prior click per user_id/live_id/streamer_id\n",
    "key_cols = [\"user_id\", \"live_id\", \"streamer_id\"]\n",
    "\n",
    "# normalize\n",
    "for c in key_cols:\n",
    "    df_click[c] = pd.to_numeric(df_click[c], errors=\"coerce\").astype(\"int64\")\n",
    "    df_like[c]  = pd.to_numeric(df_like[c],  errors=\"coerce\").astype(\"int64\")\n",
    "\n",
    "df_click[\"imp_timestamp\"] = pd.to_datetime(df_click[\"imp_timestamp\"], errors=\"coerce\")\n",
    "df_like[\"like_timestamp\"] = pd.to_datetime(df_like[\"like_timestamp\"], errors=\"coerce\")\n",
    "\n",
    "df_click_m = df_click.dropna(subset=key_cols + [\"imp_timestamp\"]).copy()\n",
    "df_like_m  = df_like.dropna(subset=key_cols + [\"like_timestamp\"]).copy()\n",
    "\n",
    "# give each click a unique id (internal only)\n",
    "df_click_m = df_click_m.sort_values(key_cols + [\"imp_timestamp\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "df_click_m[\"click_id\"] = df_click_m.index\n",
    "\n",
    "# build combined timeline\n",
    "click_t = df_click_m.rename(columns={\"imp_timestamp\": \"event_time\"})\n",
    "like_t  = df_like_m.rename(columns={\"like_timestamp\": \"event_time\"})\n",
    "\n",
    "click_t[\"_is_click\"] = 1\n",
    "like_t[\"_is_like\"] = 1\n",
    "\n",
    "combined = pd.concat([click_t, like_t], ignore_index=True, sort=False)\n",
    "combined = combined.sort_values(key_cols + [\"event_time\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "# forward-fill latest click_id within each key (so each like gets matched to latest prior click)\n",
    "combined[\"click_id\"] = combined.groupby(key_cols, sort=False)[\"click_id\"].ffill()\n",
    "\n",
    "# extract like->click mapping\n",
    "like_map = combined[combined[\"_is_like\"].eq(1)].dropna(subset=[\"click_id\"])[[\"click_id\", \"event_time\"]]\n",
    "\n",
    "# aggregate likes per click (only need latest_like_ts + is_like)\n",
    "like_agg = like_map.groupby(\"click_id\").agg(\n",
    "    latest_like_ts=(\"event_time\", \"max\")\n",
    ").reset_index()\n",
    "\n",
    "# join back to clicks (keeps ALL clicks)\n",
    "df_click_with_like = df_click_m.merge(like_agg, on=\"click_id\", how=\"left\")\n",
    "df_click_with_like[\"is_like\"] = df_click_with_like[\"latest_like_ts\"].notna().astype(\"int64\")\n",
    "\n",
    "# drop internal id\n",
    "df_click_with_like = df_click_with_like.drop(columns=[\"click_id\"])\n",
    "\n",
    "display(df_click_with_like.head())\n",
    "print(df_click_with_like.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88d9625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1.3 comment\n",
    "##### 1.3.1 Rename timestamp to comment_timestamp, Convert it to datetime feature \n",
    "##### 1.3.2 Derive is_comment\n",
    "df_comment = dataframes.get(\"comment\")\n",
    "\n",
    "if df_comment is not None:\n",
    "    df_comment = df_comment.copy()\n",
    "\n",
    "    if \"timestamp\" in df_comment.columns:\n",
    "        df_comment = df_comment.rename(columns={\"timestamp\": \"comment_timestamp\"})\n",
    "        df_comment[\"comment_timestamp\"] = pd.to_datetime(df_comment[\"comment_timestamp\"], unit=\"ms\", errors=\"coerce\")\n",
    "\n",
    "    df_comment[\"is_comment\"] = 1\n",
    "\n",
    "display(df_comment.head())\n",
    "print(df_comment.shape)\n",
    "\n",
    "\n",
    "##### 1.3.3 Merge comment data -> latest prior click per user_id/live_id/streamer_id\n",
    "key_cols = [\"user_id\", \"live_id\", \"streamer_id\"]\n",
    "\n",
    "# normalize\n",
    "for c in key_cols:\n",
    "    df_click_with_like[c] = pd.to_numeric(df_click_with_like[c], errors=\"coerce\").astype(\"int64\")\n",
    "    df_comment[c]         = pd.to_numeric(df_comment[c], errors=\"coerce\").astype(\"int64\")\n",
    "\n",
    "df_click_with_like[\"imp_timestamp\"] = pd.to_datetime(df_click_with_like[\"imp_timestamp\"], errors=\"coerce\")\n",
    "df_comment[\"comment_timestamp\"]     = pd.to_datetime(df_comment[\"comment_timestamp\"], errors=\"coerce\")\n",
    "\n",
    "df_click_m   = df_click_with_like.dropna(subset=key_cols + [\"imp_timestamp\"]).copy()\n",
    "df_comment_m = df_comment.dropna(subset=key_cols + [\"comment_timestamp\"]).copy()\n",
    "\n",
    "# give each click a unique id (internal only)\n",
    "df_click_m = df_click_m.sort_values(key_cols + [\"imp_timestamp\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "df_click_m[\"click_id\"] = df_click_m.index\n",
    "\n",
    "# build combined timeline\n",
    "click_t   = df_click_m.rename(columns={\"imp_timestamp\": \"event_time\"})\n",
    "comment_t = df_comment_m.rename(columns={\"comment_timestamp\": \"event_time\"})\n",
    "\n",
    "click_t[\"_is_click\"] = 1\n",
    "comment_t[\"_is_comment\"] = 1\n",
    "\n",
    "combined = pd.concat([click_t, comment_t], ignore_index=True, sort=False)\n",
    "combined = combined.sort_values(key_cols + [\"event_time\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "# forward-fill latest click_id within each key (so each comment gets matched to latest prior click)\n",
    "combined[\"click_id\"] = combined.groupby(key_cols, sort=False)[\"click_id\"].ffill()\n",
    "\n",
    "# extract comment->click mapping\n",
    "comment_map = combined[combined[\"_is_comment\"].eq(1)].dropna(subset=[\"click_id\"])[[\"click_id\", \"event_time\"]]\n",
    "\n",
    "# aggregate comments per click (only need latest_comment_ts + is_comment)\n",
    "comment_agg = comment_map.groupby(\"click_id\").agg(\n",
    "    latest_comment_ts=(\"event_time\", \"max\")\n",
    ").reset_index()\n",
    "\n",
    "# join back to clicks (keeps ALL clicks)\n",
    "df_click_with_like_comment = df_click_m.merge(comment_agg, on=\"click_id\", how=\"left\")\n",
    "df_click_with_like_comment[\"is_comment\"] = df_click_with_like_comment[\"latest_comment_ts\"].notna().astype(\"int64\")\n",
    "\n",
    "# drop internal id\n",
    "df_click_with_like_comment = df_click_with_like_comment.drop(columns=[\"click_id\"])\n",
    "\n",
    "display(df_click_with_like_comment.head())\n",
    "print(df_click_with_like_comment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac73c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"non-missing latest_comment_ts:\", df_click_with_like_comment[\"latest_comment_ts\"].notna().sum())\n",
    "print(\"sum(is_comment):\", df_click_with_like_comment[\"is_comment\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62911abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1.4 gift\n",
    "##### 1.4.1 Rename timestamp to gift_timestamp, Convert it to datetime feature \n",
    "##### 1.4.2 Derive is_gift\n",
    "df_gift = dataframes.get(\"gift\")\n",
    "\n",
    "if df_gift is not None:\n",
    "    df_gift = df_gift.copy()\n",
    "\n",
    "    if \"timestamp\" in df_gift.columns:\n",
    "        df_gift = df_gift.rename(columns={\"timestamp\": \"gift_timestamp\"})\n",
    "        df_gift[\"gift_timestamp\"] = pd.to_datetime(df_gift[\"gift_timestamp\"], unit=\"ms\", errors=\"coerce\")\n",
    "\n",
    "    df_gift[\"is_gift\"] = 1\n",
    "\n",
    "display(df_gift.head())\n",
    "print(df_gift.shape)\n",
    "\n",
    "\n",
    "##### 1.4.3 Merge gift data -> latest prior click per user_id/live_id/streamer_id\n",
    "key_cols = [\"user_id\", \"live_id\", \"streamer_id\"]\n",
    "\n",
    "# normalize\n",
    "for c in key_cols:\n",
    "    df_click_with_like_comment[c] = pd.to_numeric(df_click_with_like_comment[c], errors=\"coerce\").astype(\"int64\")\n",
    "    df_gift[c]                    = pd.to_numeric(df_gift[c], errors=\"coerce\").astype(\"int64\")\n",
    "\n",
    "df_click_with_like_comment[\"imp_timestamp\"] = pd.to_datetime(df_click_with_like_comment[\"imp_timestamp\"], errors=\"coerce\")\n",
    "df_gift[\"gift_timestamp\"]                   = pd.to_datetime(df_gift[\"gift_timestamp\"], errors=\"coerce\")\n",
    "\n",
    "df_click_m = df_click_with_like_comment.dropna(subset=key_cols + [\"imp_timestamp\"]).copy()\n",
    "df_gift_m  = df_gift.dropna(subset=key_cols + [\"gift_timestamp\"]).copy()\n",
    "\n",
    "# give each click a unique id (internal only)\n",
    "df_click_m = df_click_m.sort_values(key_cols + [\"imp_timestamp\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "df_click_m[\"click_id\"] = df_click_m.index\n",
    "\n",
    "# build combined timeline\n",
    "click_t = df_click_m.rename(columns={\"imp_timestamp\": \"event_time\"})\n",
    "gift_t  = df_gift_m.rename(columns={\"gift_timestamp\": \"event_time\"})\n",
    "\n",
    "click_t[\"_is_click\"] = 1\n",
    "gift_t[\"_is_gift\"] = 1\n",
    "\n",
    "combined = pd.concat([click_t, gift_t], ignore_index=True, sort=False)\n",
    "combined = combined.sort_values(key_cols + [\"event_time\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "# forward-fill latest click_id within each key (so each gift gets matched to latest prior click)\n",
    "combined[\"click_id\"] = combined.groupby(key_cols, sort=False)[\"click_id\"].ffill()\n",
    "\n",
    "# extract gift->click mapping (keep gift_price)\n",
    "gift_map = combined[combined[\"_is_gift\"].eq(1)].dropna(subset=[\"click_id\"])[\n",
    "    [\"click_id\", \"event_time\", \"gift_price\"]\n",
    "]\n",
    "\n",
    "# take the latest gift per click (by event_time), keep its price\n",
    "gift_map = gift_map.sort_values([\"click_id\", \"event_time\"], kind=\"mergesort\")\n",
    "gift_agg = gift_map.groupby(\"click_id\", as_index=False).last()\n",
    "\n",
    "gift_agg = gift_agg.rename(columns={\"event_time\": \"latest_gift_ts\"})\n",
    "\n",
    "\n",
    "# join back to clicks (keeps ALL clicks)\n",
    "df_click_with_like_comment_gift = df_click_m.merge(gift_agg, on=\"click_id\", how=\"left\")\n",
    "df_click_with_like_comment_gift[\"is_gift\"] = df_click_with_like_comment_gift[\"latest_gift_ts\"].notna().astype(\"int64\")\n",
    "\n",
    "# drop internal id\n",
    "df_click_with_like_comment_gift = df_click_with_like_comment_gift.drop(columns=[\"click_id\"])\n",
    "\n",
    "display(df_click_with_like_comment_gift.head())\n",
    "print(df_click_with_like_comment_gift.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c19b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"non-missing latest_gift_ts:\", df_click_with_like_comment_gift[\"latest_gift_ts\"].notna().sum())\n",
    "print(\"non-missing gift_price:\", df_click_with_like_comment_gift[\"gift_price\"].notna().sum())\n",
    "print(\"sum(is_gift):\", df_click_with_like_comment_gift[\"is_gift\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7942ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1.5 Negative\n",
    "##### 1.5.1 Rename timestamp to imp_timestamp, Convert it to datetime feature \n",
    "##### 1.5.2 Derive is_click, watch_live_time, is_like, is_comment, is_gift, gift_price and set their values all as 0 \n",
    "##### 1.5.3 Derive latest_like_ts, latest_comment_ts, latest_gift_ts and set as 0\n",
    "df_negative = dataframes.get(\"negative\")\n",
    "\n",
    "if df_negative is not None:\n",
    "    df_negative = df_negative.copy()\n",
    "\n",
    "    if \"timestamp\" in df_negative.columns:\n",
    "        df_negative = df_negative.rename(columns={\"timestamp\": \"imp_timestamp\"})\n",
    "        df_negative[\"imp_timestamp\"] = pd.to_datetime(df_negative[\"imp_timestamp\"], unit=\"ms\", errors=\"coerce\")\n",
    "\n",
    "    # fill required fields with 0\n",
    "    df_negative[\"is_click\"] = 0\n",
    "    df_negative[\"watch_live_time\"] = 0\n",
    "    df_negative[\"is_like\"] = 0\n",
    "    df_negative[\"is_comment\"] = 0\n",
    "    df_negative[\"is_gift\"] = 0\n",
    "    df_negative[\"gift_price\"] = 0\n",
    "\n",
    "    df_negative[\"latest_like_ts\"] = 0\n",
    "    df_negative[\"latest_comment_ts\"] = 0\n",
    "    df_negative[\"latest_gift_ts\"] = 0\n",
    "\n",
    "    display(df_negative.head())\n",
    "    print(df_negative.shape)\n",
    "\n",
    "\n",
    "##### 1.5.4 Vertically append with df_click_with_like_comment_gift to generate the final dataset \"df_interactions\"\n",
    "df_interactions = pd.concat(\n",
    "    [df_click_with_like_comment_gift, df_negative],\n",
    "    ignore_index=True,\n",
    "    sort=False\n",
    ")\n",
    "\n",
    "##### 1.5.5 Sort df_interactions by imp_timestamp, user_id, live_id, streamer_id\n",
    "df_interactions = df_interactions.sort_values(\n",
    "    [\"imp_timestamp\", \"user_id\", \"live_id\", \"streamer_id\"],\n",
    "    kind=\"mergesort\"\n",
    ").reset_index(drop=True)\n",
    "\n",
    "\n",
    "##### 1.5.6 Derive imp_year/imp_month/imp_day/imp_hour/imp_is_weekend and reorder columns\n",
    "# derive time features\n",
    "df_interactions[\"imp_year\"] = df_interactions[\"imp_timestamp\"].dt.year\n",
    "df_interactions[\"imp_month\"] = df_interactions[\"imp_timestamp\"].dt.month\n",
    "df_interactions[\"imp_day\"] = df_interactions[\"imp_timestamp\"].dt.day\n",
    "df_interactions[\"imp_hour\"] = df_interactions[\"imp_timestamp\"].dt.hour\n",
    "df_interactions[\"imp_is_weekend\"] = df_interactions[\"imp_timestamp\"].dt.weekday.ge(5).astype(\"int64\")\n",
    "\n",
    "# reorder columns: place new cols between imp_timestamp and watch_live_time\n",
    "new_cols = [\"imp_year\", \"imp_month\", \"imp_day\", \"imp_hour\", \"imp_is_weekend\"]\n",
    "cols = df_interactions.columns.tolist()\n",
    "\n",
    "imp_idx = cols.index(\"imp_timestamp\")\n",
    "watch_idx = cols.index(\"watch_live_time\")\n",
    "\n",
    "# remove new cols if already exist, then insert after imp_timestamp\n",
    "for c in new_cols:\n",
    "    cols.remove(c)\n",
    "\n",
    "cols = cols[:imp_idx + 1] + new_cols + cols[imp_idx + 1:]\n",
    "df_interactions = df_interactions[cols]\n",
    "\n",
    "# for missing values in gift_price in df_interactions, replace the missing value with zero\n",
    "df_interactions[\"gift_price\"] = df_interactions[\"gift_price\"].fillna(0)\n",
    "\n",
    "\n",
    "display(df_interactions.head())\n",
    "print(df_interactions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4706fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"count is_click == 0:\", (df_interactions[\"is_click\"] == 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb88618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missingness of each column in df_interactions\n",
    "(df_interactions.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e269e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. User Features\n",
    "#### 2.1 Convert reg_timestamp and first_watch_live_timestamp to datetime features\n",
    "#### 2.2 rename features \n",
    "    # 1) age -->  user_age\n",
    "    # 2) gender --> user_gender\n",
    "    # 3) country --> user_country\n",
    "    # 4) device_brand --> user_device_brand\n",
    "    # 5) device_price --> user_device_price\n",
    "    # 6) reg_timestamp --> user_reg_timestamp\n",
    "    # 7) onehot_feat0 --- onehot_feat6 --> user_onehot_feat0 --- user_onehot_feat6\n",
    "\n",
    "df_user = dataframes.get(\"user\")\n",
    "\n",
    "if df_user is not None:\n",
    "\n",
    "    df_user = df_user.copy()\n",
    "    \n",
    "    # check duplicates by user_id\n",
    "    if \"user_id\" in df_user.columns:\n",
    "        dup_mask = df_user.duplicated(subset=[\"user_id\"], keep=False)\n",
    "        dup_count = int(dup_mask.sum())\n",
    "        print(f\"df_user duplicate rows by user_id: {dup_count}\")\n",
    "        if dup_count:\n",
    "            display(df_user.loc[dup_mask].sort_values(\"user_id\").head())\n",
    "    else:\n",
    "        print(\"df_user duplicate rows by user_id: user_id column not found\")\n",
    "\n",
    "\n",
    "    # convert date-like strings to datetime\n",
    "    df_user[\"reg_timestamp\"] = pd.to_datetime(df_user[\"reg_timestamp\"], errors=\"coerce\")\n",
    "    df_user[\"first_watch_live_timestamp\"] = pd.to_datetime(df_user[\"first_watch_live_timestamp\"], errors=\"coerce\")\n",
    "\n",
    "    rename_map = {\n",
    "        \"age\": \"user_age\",\n",
    "        \"gender\": \"user_gender\",\n",
    "        \"country\": \"user_country\",\n",
    "        \"device_brand\": \"user_device_brand\",\n",
    "        \"device_price\": \"user_device_price\",\n",
    "        \"reg_timestamp\": \"user_reg_timestamp\",\n",
    "    }\n",
    "    rename_map.update({f\"onehot_feat{i}\": f\"user_onehot_feat{i}\" for i in range(7)})\n",
    "\n",
    "    df_user = df_user.rename(columns=rename_map)\n",
    "\n",
    "display(df_user.head())\n",
    "print(df_user.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b75f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missingness of each column in df_user\n",
    "(df_user.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fa18c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect unique counts and values for selected user features\n",
    "cols = [\n",
    "    \"user_age\",\n",
    "    \"user_gender\",\n",
    "    \"user_country\",\n",
    "    \"user_device_brand\",\n",
    "    \"user_device_price\",\n",
    "    \"fans_num\",\n",
    "    \"follow_num\",\n",
    "    \"accu_watch_live_cnt\",\n",
    "    \"accu_watch_live_duration\",\n",
    "]\n",
    "\n",
    "for c in cols:\n",
    "    uniq = df_user[c].dropna().unique()\n",
    "    print(f\"{c}: n_unique={len(uniq)}\")\n",
    "    print(uniq)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2bd7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2.3 Label encoding\n",
    "\t\t# 1) user_age --> user_age_le\n",
    "        # 2) user_gender --> user_gender_le\n",
    "\t\t# 3) country --> user_country_le\n",
    "\t\t# 4) device_brand --> user_device_brand_le\n",
    "\t\t# 5) device_price --> user_device_price_le\n",
    "\t\t# 6) fans_num --> fans_num_le\n",
    "        # 7) follow_num --> follow_num_le\n",
    "\t\t# 8) accu_watch_live_cnt --> accu_watch_live_cnt_le\n",
    "\t\t# 9) accu_watch_live_duration --> accu_watch_live_duration_le\n",
    "\n",
    "le_cols = [\n",
    "    \"user_age\",\n",
    "    \"user_gender\",\n",
    "    \"user_country\",\n",
    "    \"user_device_brand\",\n",
    "    \"user_device_price\",\n",
    "    \"fans_num\",\n",
    "    \"follow_num\",\n",
    "    \"accu_watch_live_cnt\",\n",
    "    \"accu_watch_live_duration\",\n",
    "]\n",
    "\n",
    "for c in le_cols:\n",
    "    codes, _ = pd.factorize(df_user[c], sort=True)\n",
    "    df_user[f\"{c}_le\"] = codes\n",
    "\n",
    "\n",
    "with pd.option_context(\"display.max_columns\", None, \"display.width\", None):\n",
    "    display(df_user.head())\n",
    "print(df_user.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ccafea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missingness of each column in df_user\n",
    "(df_user.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d475b171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect unique counts and values for selected user features\n",
    "cols = [\n",
    "    \"user_age_le\",\n",
    "    \"user_country_le\",\n",
    "    \"user_device_brand_le\",\n",
    "    \"user_device_price_le\",\n",
    "    \"fans_num_le\",\n",
    "    \"follow_num_le\",\n",
    "    \"accu_watch_live_cnt_le\",\n",
    "    \"accu_watch_live_duration_le\",\n",
    "]\n",
    "\n",
    "for c in cols:\n",
    "    uniq = df_user[c].dropna().unique()\n",
    "    print(f\"{c}: n_unique={len(uniq)}\")\n",
    "    print(uniq)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7240fab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Room features\n",
    "#### 3.0 remove duplicate live_id, room_id\n",
    "#### 3.1 Convert p_date, start_timestamp and end_timestamp to datetime features\n",
    "#### 3.2 Label encoding \n",
    "\t\t# live_content_category --> see unique values and missingness ---> label encoding --> live_content_category_le\n",
    "#### 3.3 New features\n",
    "\t\t# xxxx1) time_since_live_sart (ms) = imp_timestamp - start_timestamp (Pending after merge with interaction)\n",
    "\t\t# 2) live_start_year\n",
    "\t\t# 3) live_start_month\n",
    "\t\t# 4) live_start_day\n",
    "\t\t# 5) live_start_hour\n",
    "\t\t# 6) live_is_weekend\n",
    "#### 3.4 merge title_embedding dataset by live_name_id\n",
    "#### 3.5 Fill missing embedding with 0 and derive a flag variable title_emb_missing\n",
    "#### 3.6 check missing of all variables\n",
    "\n",
    "### 3. Room features\n",
    "#### 3.0 remove duplicate live_id, room_id\n",
    "#### 3.1 Convert p_date, start_timestamp and end_timestamp to datetime features\n",
    "df_room = dataframes.get(\"room\")\n",
    "\n",
    "if df_room is not None:\n",
    "    df_room = df_room.copy()\n",
    "\n",
    "    # check duplicates by room_id + streamer_id\n",
    "    key_cols = [\"live_id\", \"streamer_id\"]\n",
    "    if all(c in df_room.columns for c in key_cols):\n",
    "        dup_mask = df_room.duplicated(subset=key_cols, keep=False)\n",
    "        dup_count = int(dup_mask.sum())\n",
    "        print(f\"df_room duplicate rows by {key_cols}: {dup_count}\")\n",
    "        if dup_count:\n",
    "            display(df_room.loc[dup_mask].sort_values(key_cols).head())\n",
    "    else:\n",
    "        print(f\"df_room duplicate rows by {key_cols}: keys not found\")\n",
    "\n",
    "    df_room[\"p_date\"] = pd.to_datetime(df_room[\"p_date\"].astype(str), format=\"%Y%m%d\", errors=\"coerce\")\n",
    "    df_room[\"start_timestamp\"] = pd.to_datetime(df_room[\"start_timestamp\"], unit=\"ms\", errors=\"coerce\")\n",
    "    df_room[\"end_timestamp\"] = pd.to_datetime(df_room[\"end_timestamp\"], unit=\"ms\", errors=\"coerce\")\n",
    "\n",
    "display(df_room.head())\n",
    "print(df_room.shape)\n",
    "# check missingness of each column in df_user\n",
    "(df_room.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d942b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3.2 Label encoding \n",
    "# live_content_category --> see unique values and missingness ---> label encoding --> live_content_category_le\n",
    "print(\"live_content_category missing:\", df_room[\"live_content_category\"].isna().sum())\n",
    "print(\"live_content_category unique:\", df_room[\"live_content_category\"].nunique(dropna=True))\n",
    "print(df_room[\"live_content_category\"].dropna().unique())\n",
    "\n",
    "codes, _ = pd.factorize(df_room[\"live_content_category\"], sort=True)\n",
    "df_room[\"live_content_category_le\"] = codes\n",
    "\n",
    "\n",
    "#### 3.3 New features \n",
    "# 1) live_start_year\n",
    "# 2) live_start_month\n",
    "# 3) live_start_day\n",
    "# 4) live_start_hour\n",
    "# 5) live_is_weekend\n",
    "df_room[\"live_start_year\"] = df_room[\"start_timestamp\"].dt.year\n",
    "df_room[\"live_start_month\"] = df_room[\"start_timestamp\"].dt.month\n",
    "df_room[\"live_start_day\"] = df_room[\"start_timestamp\"].dt.day\n",
    "df_room[\"live_start_hour\"] = df_room[\"start_timestamp\"].dt.hour\n",
    "df_room[\"live_is_weekend\"] = df_room[\"start_timestamp\"].dt.weekday.ge(5).astype(\"int64\")\n",
    "\n",
    "display(df_room.head())\n",
    "print(df_room.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"min live_name_id:\", df_room[\"live_name_id\"].min())\n",
    "print(\"max live_name_id:\", df_room[\"live_name_id\"].max())\n",
    "(df_room[\"live_name_id\"] == -1).sum()\n",
    "# note about 1834913 (15%) records in df_room have live_name_id == -1, indicating they miss title_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6afe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3.4 merge title_embedding dataset by live_name_id\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# use existing df_room (do NOT reload or overwrite)\n",
    "assert df_room is not None\n",
    "\n",
    "# load embeddings\n",
    "data_dir = Path(\"..\") / \"data\"\n",
    "emb = np.load(data_dir / \"title_embeddings.npy\")\n",
    "\n",
    "# build live_name_id list aligned to embeddings (exclude -1 for alignment only)\n",
    "live_ids = (\n",
    "    df_room[\"live_name_id\"]\n",
    "    .dropna()\n",
    "    .astype(\"int64\")\n",
    "    .sort_values()\n",
    "    .unique()\n",
    ")\n",
    "live_ids_for_emb = live_ids[live_ids != -1]\n",
    "\n",
    "# ensure counts match\n",
    "assert len(live_ids_for_emb) == emb.shape[0]\n",
    "\n",
    "df_title_embedding = pd.DataFrame(\n",
    "    emb, columns=[f\"title_emb_{i}\" for i in range(emb.shape[1])]\n",
    ")\n",
    "df_title_embedding[\"live_name_id\"] = live_ids_for_emb\n",
    "\n",
    "# merge into existing df_room (keeps derived columns)\n",
    "df_room = df_room.merge(df_title_embedding, on=\"live_name_id\", how=\"left\")\n",
    "\n",
    "\n",
    "#### 3.5 Fill missing embedding with 0 and derive a flag variable title_emb_missing\n",
    "emb_cols = [c for c in df_room.columns if c.startswith(\"title_emb_\")]\n",
    "\n",
    "df_room[\"title_emb_missing\"] = df_room[emb_cols].isna().any(axis=1).astype(\"int64\")\n",
    "df_room[emb_cols] = df_room[emb_cols].fillna(0)\n",
    "\n",
    "\n",
    "display(\"df_title_embedding:\",df_title_embedding.head())\n",
    "print(\"df_title_embedding:\",df_title_embedding.shape)\n",
    "display(\"df_room:\", df_room.head())\n",
    "print(\"df_room:\",df_room.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b79a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: pick one live_name_id per live_id\n",
    "# Rule: most frequent live_name_id; if tied, pick the one with latest start_timestamp\n",
    "\n",
    "tmp = df_room[[\"live_id\", \"live_name_id\", \"start_timestamp\"]].dropna(subset=[\"live_id\", \"live_name_id\"])\n",
    "\n",
    "counts = (\n",
    "    tmp.groupby([\"live_id\", \"live_name_id\"], as_index=False)\n",
    "       .agg(freq=(\"live_name_id\", \"size\"), latest_ts=(\"start_timestamp\", \"max\"))\n",
    ")\n",
    "\n",
    "best = (\n",
    "    counts.sort_values([\"live_id\", \"freq\", \"latest_ts\"], ascending=[True, False, False])\n",
    "          .drop_duplicates(subset=[\"live_id\"], keep=\"first\")\n",
    "          .rename(columns={\"live_name_id\": \"live_name_id_mostfreq\"})\n",
    "          [[\"live_id\", \"live_name_id_mostfreq\"]]\n",
    ")\n",
    "\n",
    "df_room = df_room.merge(best, on=\"live_id\", how=\"left\")\n",
    "\n",
    "# overwrite live_name_id with chosen one\n",
    "df_room[\"live_name_id\"] = df_room[\"live_name_id_mostfreq\"]\n",
    "\n",
    "# now drop duplicate rows: keep the latest start_timestamp per live_id\n",
    "df_room = (\n",
    "    df_room.sort_values([\"live_id\", \"start_timestamp\"], ascending=[True, False])\n",
    "           .drop_duplicates(subset=[\"live_id\"], keep=\"first\")\n",
    ")\n",
    "\n",
    "# cleanup\n",
    "df_room = df_room.drop(columns=[\"live_name_id_mostfreq\"], errors=\"ignore\")\n",
    "for v in [\"tmp\", \"counts\", \"best\"]:\n",
    "    if v in globals():\n",
    "        del globals()[v]\n",
    "\n",
    "display(df_room.head())\n",
    "print(df_room.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3378d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify df_room size, embedding coverage, unique IDs, and column missingness\n",
    "print(\"rows in df_room:\", len(df_room))\n",
    "print(\"title_emb_missing flagged:\", df_room[\"title_emb_missing\"].sum())\n",
    "print(\"unique live_name_id:\", df_room[\"live_name_id\"].nunique(dropna=True))\n",
    "# check live_name_id range\n",
    "print(\"min live_name_id:\", df_room[\"live_name_id\"].min())\n",
    "print(\"max live_name_id:\", df_room[\"live_name_id\"].max())\n",
    "\n",
    "missing_room = pd.DataFrame({\n",
    "    \"missing_count\": df_room.isna().sum(),\n",
    "    \"missing_pct\": df_room.isna().mean().mul(100)\n",
    "}).sort_values(\"missing_count\", ascending=False)\n",
    "\n",
    "display(missing_room)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3627fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_columns\", None, \"display.width\", None):\n",
    "    display(df_room.head())\n",
    "print(\"df_room:\",df_room.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cb60d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. streamer features\n",
    "#### 4.1 Convert reg_timestamp and first_live_timestamp to datetime features\n",
    "#### 4.2 Rename\n",
    "# 1) age -->  streamer_age\n",
    "# 2) gender --> streamer_gender\n",
    "# 3) country --> streamer_country\n",
    "# 4) device_brand --> streamer_device_brand\n",
    "# 5) device_price --> streamer_device_price\n",
    "# 6) reg_timestamp --> streamer_reg_timestamp\n",
    "# 7) onehot_feat0 --- onehot_feat6 --> streamer_onehot_feat0 --- streamer_onehot_feat6\n",
    "\n",
    "df_streamer = dataframes.get(\"streamer\")\n",
    "\n",
    "if df_streamer is not None:\n",
    "    df_streamer = df_streamer.copy()\n",
    "\n",
    "    # check duplicates by streamer_id\n",
    "    if \"streamer_id\" in df_streamer.columns:\n",
    "        dup_mask = df_streamer.duplicated(subset=[\"streamer_id\"], keep=False)\n",
    "        dup_count = int(dup_mask.sum())\n",
    "        print(f\"df_streamer duplicate rows by streamer_id: {dup_count}\")\n",
    "        if dup_count:\n",
    "            display(df_streamer.loc[dup_mask].sort_values(\"streamer_id\").head())\n",
    "    else:\n",
    "        print(\"df_streamer duplicate rows by streamer_id: streamer_id column not found\")\n",
    "\n",
    "    df_streamer[\"reg_timestamp\"] = pd.to_datetime(df_streamer[\"reg_timestamp\"], errors=\"coerce\")\n",
    "    df_streamer[\"first_live_timestamp\"] = pd.to_datetime(df_streamer[\"first_live_timestamp\"], errors=\"coerce\")\n",
    "\n",
    "    rename_map = {\n",
    "        \"age\": \"streamer_age\",\n",
    "        \"gender\": \"streamer_gender\",\n",
    "        \"country\": \"streamer_country\",\n",
    "        \"device_brand\": \"streamer_device_brand\",\n",
    "        \"device_price\": \"streamer_device_price\",\n",
    "        \"reg_timestamp\": \"streamer_reg_timestamp\",\n",
    "    }\n",
    "    rename_map.update({f\"onehot_feat{i}\": f\"streamer_onehot_feat{i}\" for i in range(7)})\n",
    "\n",
    "    df_streamer = df_streamer.rename(columns=rename_map)\n",
    "\n",
    "\n",
    "display(\"df_streamer:\",df_streamer.head())\n",
    "print(\"df_streamer:\", df_streamer.shape)\n",
    "\n",
    "#### 4.3 Label Encoding\n",
    "le_cols = [\n",
    "    \"streamer_age\",\n",
    "    \"streamer_gender\",      # pending if you want special handling\n",
    "    \"streamer_country\",\n",
    "    \"streamer_device_brand\",\n",
    "    \"streamer_device_price\",\n",
    "    \"live_operation_tag\",\n",
    "    \"fans_user_num\",\n",
    "    \"fans_group_fans_num\",\n",
    "    \"follow_user_num\",\n",
    "    \"accu_live_cnt\",\n",
    "    \"accu_live_duration\",\n",
    "    \"accu_play_cnt\",\n",
    "    \"accu_play_duration\",\n",
    "]\n",
    "\n",
    "# check unique values + missingness, then label encode\n",
    "for c in le_cols:\n",
    "    if c in df_streamer.columns:\n",
    "        print(f\"{c}: missing={df_streamer[c].isna().sum()}, unique={df_streamer[c].nunique(dropna=True)}\")\n",
    "        codes, _ = pd.factorize(df_streamer[c], sort=True)\n",
    "        df_streamer[f\"{c}_le\"] = codes\n",
    "\n",
    "\n",
    "#### 4.4 check missing of all variables\n",
    "missing_streamer = pd.DataFrame({\n",
    "    \"missing_count\": df_streamer.isna().sum(),\n",
    "    \"missing_pct\": df_streamer.isna().mean().mul(100)\n",
    "}).sort_values(\"missing_count\", ascending=False)\n",
    "\n",
    "display(missing_streamer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3117942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"df_streamer:\",df_streamer.head())\n",
    "print(\"df_streamer:\", df_streamer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec88c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. concatenation\n",
    "#### 5.1 merge df_room + df_streamer by streamer_id --> df_room_streamer\n",
    "df_room_streamer = df_room.merge(df_streamer, on=\"streamer_id\", how=\"left\")\n",
    "\n",
    "print(\"df_room_streamer:\", df_room_streamer.shape)\n",
    "\n",
    "#### 5.2 merge df_interactions + df_user by user_id--> df_interaction_user\n",
    "df_interaction_user = df_interactions.merge(df_user, on=\"user_id\", how=\"left\")\n",
    "\n",
    "print(\"df_interaction_user:\", df_interaction_user.shape)\n",
    "\n",
    "#### 5.3 merge df_interaction_user + df_room_streamer by streamer_id and live_id\n",
    "df_final = df_interaction_user.merge(\n",
    "    df_room_streamer,\n",
    "    on=[\"streamer_id\", \"live_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"df_final:\", df_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e55786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicates on (streamer_id, live_id) in df_room_streamer\n",
    "dup_mask = df_room_streamer.duplicated(subset=[\"streamer_id\", \"live_id\"], keep=False)\n",
    "dup_rows = df_room_streamer[dup_mask]\n",
    "\n",
    "print(\"duplicate row count:\", int(dup_mask.sum()))\n",
    "print(\"unique duplicate keys:\", dup_rows[[\"streamer_id\", \"live_id\"]].drop_duplicates().shape[0])\n",
    "\n",
    "# preview duplicate keys + a few rows\n",
    "# show full columns for duplicate rows (first 10)\n",
    "with pd.option_context(\"display.max_columns\", None, \"display.width\", None):\n",
    "    display(dup_rows.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dbc05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. Other derivation (Contextual / Temporal / Cross Features)\n",
    "#### 6.1 User Features\n",
    "##### 6.1.1 Basic\n",
    "\t\t\t# 1) user_account_age = imp_timestamp - user_reg_timestamp\n",
    "\t\t\t# 2) user_watch_live_age = imp_timestamp - first_watch_live_timestamp\n",
    "##### 6.1.2 User CTR (pre-impression, denominator: impressions)\n",
    "\t\t\t# ctr_user_15min\n",
    "\t\t\t# ctr_user_3hr\n",
    "\t\t\t# ctr_user_1d\n",
    "\t\t\t# ctr_user_7d\n",
    "##### 6.1.3 User exposure fatigue - Imp\n",
    "\t\t\t# num_imp_user_10min\n",
    "\t\t\t# num_imp_user_30min\n",
    "\t\t\t# num_imp_user_2hr\n",
    "\t\t\t# num_imp_user_12hr\n",
    "\t\t\t# num_imp_user_1d\n",
    "\t\t\t# num_imp_user_7d\n",
    "##### 6.1.4 User click fatigue - click\n",
    "\t\t\t# num_click_user_15min\n",
    "\t\t\t# num_click_user_3hr\n",
    "\t\t\t# num_click_user_1d\n",
    "\t\t\t# num_click_user_7d\n",
    "\t\t\t# click_trend_user = log(num_click_user_15min + 1) - log(num_click_user_3hr + 1)\n",
    "##### 6.1.5 User recency\n",
    "\t\t\t# time_since_last_impression_user\n",
    "\t\t\t# tsli_missing - 1 if the user has no prior impression (first impression); 0 - otherwise\n",
    "\t\t\t# time_since_last_click_user\n",
    "\t\t\t# tslc_missing - - 1 if the user has no reliable prior click (never clicked or click masked by causality guard); 0 - otherwise\n",
    "\t\t\t# consecutive_skips_user: number of impressions since last click\n",
    "##### 6.1.6 User dwell / engagement quality (from past clicks only)\n",
    "\t\t\t# avg_watch_time_user\n",
    "\t\t\t# avg_watch_time_user\n",
    "\t\t\t# median_watch_time_user\n",
    "\t\t\t# median_watch_time_user\n",
    "\t\t\t# pct_long_watch_user_30s\n",
    "##### 6.1.7 User comment behavior (Denominator: clicks)\n",
    "\t\t\t# comment_rate_user = (num_comment_user + 1) / (num_click_user + 1)\n",
    "\t\t\t# has_comment_user_24h\n",
    "\t\t\t# num_comment_user_24h\n",
    "##### 6.1.8 User like behavior (Denominator: clicks)\n",
    "\t\t\t# like_rate_user = (num_like_user + 1) / (num_click_user + 1)\n",
    "\t\t\t# has_like_user_24h\n",
    "\t\t\t# num_like_user_24h\n",
    "##### 6.1.9 User gift behavior\n",
    "\t\t\t# has_gift_user_7d\n",
    "\t\t\t# num_gift_user_7d\n",
    "\t\t\t# amount_gift_user_7d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edf8ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_sample = df_final.sample(frac=0.02, random_state=42).copy()\n",
    "df_final_sample = df_final_sample.reset_index(drop=True)\n",
    "df_final_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 6.1.1 Basic\n",
    "# user_account_age (days) = imp_timestamp - user_reg_timestamp\n",
    "# user_watch_live_age (days) = imp_timestamp - first_watch_live_timestamp\n",
    "df_final_sample[\"user_account_age\"] = (\n",
    "    (df_final_sample[\"imp_timestamp\"] - df_final_sample[\"user_reg_timestamp\"])\n",
    "    .dt.total_seconds() / 86400\n",
    ")\n",
    "\n",
    "df_final_sample[\"user_watch_live_age\"] = (\n",
    "    (df_final_sample[\"imp_timestamp\"] - df_final_sample[\"first_watch_live_timestamp\"])\n",
    "    .dt.total_seconds() / 86400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a477196",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"live_id\",\n",
    "    \"streamer_id\",\n",
    "    \"imp_timestamp\",\n",
    "    \"user_reg_timestamp\",\n",
    "    \"first_watch_live_timestamp\",\n",
    "    \"user_account_age\",\n",
    "    \"user_watch_live_age\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 6.1.2 User CTR (pre-impression, denominator: impressions)\n",
    "df_final_sample = df_final_sample.sort_values([\"user_id\", \"imp_timestamp\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "g = df_final_sample.groupby(\"user_id\", group_keys=False)\n",
    "\n",
    "windows = {\n",
    "    \"15min\": \"ctr_user_15min\",\n",
    "    \"3h\": \"ctr_user_3hr\",\n",
    "    \"1d\": \"ctr_user_1d\",\n",
    "    \"7d\": \"ctr_user_7d\",\n",
    "}\n",
    "\n",
    "for w, col in windows.items():\n",
    "    clicks = (\n",
    "        g.rolling(w, on=\"imp_timestamp\", closed=\"left\")[\"is_click\"]\n",
    "        .sum()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    imps = (\n",
    "        g.rolling(w, on=\"imp_timestamp\", closed=\"left\")[\"is_click\"]\n",
    "        .count()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    df_final_sample[col] = (clicks / imps.replace(0, np.nan)).fillna(0).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f539c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"live_id\",\n",
    "    \"streamer_id\",\n",
    "    \"ctr_user_15min\",\n",
    "    \"ctr_user_3hr\",\n",
    "    \"ctr_user_1d\",\n",
    "    \"ctr_user_7d\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "\n",
    "cols = [\"ctr_user_15min\", \"ctr_user_3hr\", \"ctr_user_1d\", \"ctr_user_7d\"]\n",
    "\n",
    "# missingness\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[cols].isna().mean().mul(100)\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "# distribution summary\n",
    "display(df_final_sample[cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3841a8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 6.1.3 User exposure fatigue - Imp (on df_final_sample)\n",
    "\n",
    "# ensure datetime + stable ordering\n",
    "df_final_sample[\"imp_timestamp\"] = pd.to_datetime(df_final_sample[\"imp_timestamp\"], errors=\"coerce\")\n",
    "df_final_sample = df_final_sample.sort_values([\"user_id\", \"imp_timestamp\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "g = df_final_sample.groupby(\"user_id\", group_keys=False)\n",
    "\n",
    "imp_windows = {\n",
    "    \"10min\": \"num_imp_user_10min\",\n",
    "    \"30min\": \"num_imp_user_30min\",\n",
    "    \"2h\": \"num_imp_user_2hr\",\n",
    "    \"12h\": \"num_imp_user_12hr\",\n",
    "    \"1d\": \"num_imp_user_1d\",\n",
    "    \"7d\": \"num_imp_user_7d\",\n",
    "}\n",
    "\n",
    "for w, col in imp_windows.items():\n",
    "    imps = (\n",
    "        g.rolling(w, on=\"imp_timestamp\", closed=\"left\")[\"is_click\"]\n",
    "        .count()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    df_final_sample[col] = imps.to_numpy()\n",
    "\n",
    "# optional: fill missing with 0\n",
    "df_final_sample[list(imp_windows.values())] = df_final_sample[list(imp_windows.values())].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5a9791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check new 6.1.3 features (user exposure fatigue)\n",
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"live_id\",\n",
    "    \"streamer_id\",\n",
    "    \"num_imp_user_10min\",\n",
    "    \"num_imp_user_30min\",\n",
    "    \"num_imp_user_2hr\",\n",
    "    \"num_imp_user_12hr\",\n",
    "    \"num_imp_user_1d\",\n",
    "    \"num_imp_user_7d\"\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "print(df_final_sample[cols].shape)\n",
    "\n",
    "feat_cols = [\n",
    "    \"num_imp_user_10min\",\n",
    "    \"num_imp_user_30min\",\n",
    "    \"num_imp_user_2hr\",\n",
    "    \"num_imp_user_12hr\",\n",
    "    \"num_imp_user_1d\",\n",
    "    \"num_imp_user_7d\"\n",
    "]\n",
    "\n",
    "# missingness\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100)\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "# distribution summary\n",
    "display(df_final_sample[feat_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116e0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 6.1.4 User click fatigue - click (on df_final_sample)\n",
    "df_final_sample = df_final_sample.sort_values([\"user_id\", \"imp_timestamp\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "g = df_final_sample.groupby(\"user_id\", group_keys=False)\n",
    "\n",
    "click_windows = {\n",
    "    \"15min\": \"num_click_user_15min\",\n",
    "    \"3h\": \"num_click_user_3hr\",\n",
    "    \"1d\": \"num_click_user_1d\",\n",
    "    \"7d\": \"num_click_user_7d\",\n",
    "}\n",
    "\n",
    "for w, col in click_windows.items():\n",
    "    clicks = (\n",
    "        g.rolling(w, on=\"imp_timestamp\", closed=\"left\")[\"is_click\"]\n",
    "        .sum()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    df_final_sample[col] = clicks.to_numpy()\n",
    "\n",
    "# trend feature\n",
    "df_final_sample[\"click_trend_user\"] = (\n",
    "    np.log(df_final_sample[\"num_click_user_15min\"] + 1)\n",
    "    - np.log(df_final_sample[\"num_click_user_3hr\"] + 1)\n",
    ")\n",
    "\n",
    "# optional: fill missing with 0\n",
    "df_final_sample[list(click_windows.values()) + [\"click_trend_user\"]] = (\n",
    "    df_final_sample[list(click_windows.values()) + [\"click_trend_user\"]].fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092b96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"live_id\",\n",
    "    \"streamer_id\",\n",
    "    \"num_click_user_15min\",\n",
    "    \"num_click_user_3hr\",\n",
    "    \"num_click_user_1d\",\n",
    "    \"num_click_user_7d\",\n",
    "    \"click_trend_user\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "feat_cols = [\n",
    "    \"num_click_user_15min\",\n",
    "    \"num_click_user_3hr\",\n",
    "    \"num_click_user_1d\",\n",
    "    \"num_click_user_7d\",\n",
    "    \"click_trend_user\",\n",
    "]\n",
    "\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100)\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "display(df_final_sample[feat_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d143c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 6.1.5 User recency (on df_final_sample)  milliseconds\n",
    "df_final_sample = (\n",
    "    df_final_sample.sort_values([\"user_id\", \"imp_timestamp\"], kind=\"mergesort\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "g = df_final_sample.groupby(\"user_id\", group_keys=False)\n",
    "\n",
    "# 1) time_since_last_impression_user (ms)\n",
    "df_final_sample[\"time_since_last_impression_user\"] = (\n",
    "    g[\"imp_timestamp\"].diff().dt.total_seconds().mul(1000)\n",
    ")\n",
    "\n",
    "# 2) time_since_last_click_user (ms)  prior click only, causality-guarded\n",
    "last_click_time = (\n",
    "    df_final_sample[\"imp_timestamp\"]\n",
    "    .where(df_final_sample[\"is_click\"] == 1)\n",
    "    .groupby(df_final_sample[\"user_id\"])\n",
    "    .ffill()\n",
    "    .shift(1)\n",
    ")\n",
    "\n",
    "# guard: do not allow \"last click\" to be in the future\n",
    "last_click_time = last_click_time.where(last_click_time <= df_final_sample[\"imp_timestamp\"])\n",
    "\n",
    "df_final_sample[\"time_since_last_click_user\"] = (\n",
    "    (df_final_sample[\"imp_timestamp\"] - last_click_time)\n",
    "    .dt.total_seconds()\n",
    "    .mul(1000)\n",
    ")\n",
    "\n",
    "# 3) consecutive_skips_user: #imps since last click (click row -> 0)\n",
    "click_group = g[\"is_click\"].cumsum()\n",
    "df_final_sample[\"consecutive_skips_user\"] = (\n",
    "    df_final_sample.groupby([\"user_id\", click_group]).cumcount()\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Missing handling for NN / DCNv2\n",
    "# -------------------------------\n",
    "# Missing indicators (high-signal for cold-start / no history)\n",
    "df_final_sample[\"tsli_missing\"] = df_final_sample[\"time_since_last_impression_user\"].isna().astype(np.int8)\n",
    "df_final_sample[\"tslc_missing\"] = df_final_sample[\"time_since_last_click_user\"].isna().astype(np.int8)\n",
    "\n",
    "# Sentinel fill (use a large \"no/very old history\" value)\n",
    "TSLI_FILL_MS = 7 * 24 * 3600 * 1000     # 7 days\n",
    "TSLC_FILL_MS = 30 * 24 * 3600 * 1000    # 30 days\n",
    "\n",
    "df_final_sample[\"time_since_last_impression_user\"] = (\n",
    "    df_final_sample[\"time_since_last_impression_user\"].fillna(TSLI_FILL_MS)\n",
    ")\n",
    "df_final_sample[\"time_since_last_click_user\"] = (\n",
    "    df_final_sample[\"time_since_last_click_user\"].fillna(TSLC_FILL_MS)\n",
    ")\n",
    "\n",
    "# Optional but recommended: log transform to reduce scale / heavy tails\n",
    "# df_final_sample[\"time_since_last_impression_user\"] = np.log1p(df_final_sample[\"time_since_last_impression_user\"])\n",
    "# df_final_sample[\"time_since_last_click_user\"] = np.log1p(df_final_sample[\"time_since_last_click_user\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda20d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 6.1.5 user recency features\n",
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"live_id\",\n",
    "    \"streamer_id\",\n",
    "    \"imp_timestamp\",\n",
    "    \"time_since_last_impression_user\",\n",
    "    \"time_since_last_click_user\",\n",
    "    \"consecutive_skips_user\",\n",
    "    \"tsli_missing\",\n",
    "    \"tslc_missing\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "feat_cols = [\n",
    "    \"time_since_last_impression_user\",\n",
    "    \"time_since_last_click_user\",\n",
    "    \"consecutive_skips_user\",\n",
    "    \"tsli_missing\",\n",
    "    \"tslc_missing\",\n",
    "]\n",
    "\n",
    "# missingness\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100)\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "# distribution summary\n",
    "display(df_final_sample[feat_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2021d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking code (to prove the problem + verify the fix)\n",
    "# A) How many future last click cases exist?\n",
    "future_last_click = last_click_time > df_final_sample[\"imp_timestamp\"]\n",
    "print(\"future_last_click rows:\", int(future_last_click.sum()))\n",
    "\n",
    "# B) Any negatives after the guard?\n",
    "neg = df_final_sample[\"time_since_last_click_user\"] < 0\n",
    "print(\"negative time_since_last_click_user rows:\", int(neg.sum()))\n",
    "\n",
    "# C) Show a bad users timeline (if any future cases still exist)\n",
    "if future_last_click.any():\n",
    "    uid = df_final_sample.loc[future_last_click, \"user_id\"].iloc[0]\n",
    "    tmp = df_final_sample[df_final_sample[\"user_id\"] == uid].copy()\n",
    "    tmp[\"last_click_time_used\"] = last_click_time.loc[tmp.index]\n",
    "    tmp[\"future_last_click\"] = tmp[\"last_click_time_used\"] > tmp[\"imp_timestamp\"]\n",
    "    tmp[[\"imp_timestamp\",\"is_click\",\"last_click_time_used\",\"future_last_click\",\"time_since_last_click_user\"]].tail(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562281d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 6.1.6 User dwell / engagement quality (from past clicks only)\n",
    "df_final_sample = df_final_sample.sort_values([\"user_id\", \"imp_timestamp\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "# consider watch time only on clicks\n",
    "df_final_sample[\"_watch_on_click\"] = df_final_sample[\"watch_live_time\"].where(df_final_sample[\"is_click\"] == 1)\n",
    "\n",
    "g = df_final_sample.groupby(\"user_id\", group_keys=False)\n",
    "\n",
    "# expanding stats using past clicks only (shift by 1 to avoid leakage)\n",
    "df_final_sample[\"avg_watch_time_user\"] = (\n",
    "    g[\"_watch_on_click\"].expanding().mean().shift(1).reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "df_final_sample[\"median_watch_time_user\"] = (\n",
    "    g[\"_watch_on_click\"].expanding().median().shift(1).reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# pct_long_watch_user_30s: past clicks with watch_live_time >= 30\n",
    "past_clicks = g[\"is_click\"].expanding().sum().shift(1).reset_index(level=0, drop=True)\n",
    "past_long = (\n",
    "    g[\"_watch_on_click\"].apply(lambda s: (s >= 30).expanding().sum())\n",
    "    .shift(1)\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "df_final_sample[\"pct_long_watch_user_30s\"] = (past_long / past_clicks.replace(0, np.nan)).fillna(0)\n",
    "\n",
    "# cleanup helper\n",
    "df_final_sample = df_final_sample.drop(columns=[\"_watch_on_click\"])\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Missing handling for NN / DCNv2\n",
    "# -------------------------------\n",
    "\n",
    "# Missing indicators: no past clicked watch history\n",
    "df_final_sample[\"avg_watch_time_user_missing\"] = df_final_sample[\"avg_watch_time_user\"].isna().astype(np.int8)\n",
    "df_final_sample[\"median_watch_time_user_missing\"] = df_final_sample[\"median_watch_time_user\"].isna().astype(np.int8)\n",
    "\n",
    "# Fill NaNs with 0 (paired with missing flags, so semantics are preserved)\n",
    "df_final_sample[\"avg_watch_time_user\"] = df_final_sample[\"avg_watch_time_user\"].fillna(0.0)\n",
    "df_final_sample[\"median_watch_time_user\"] = df_final_sample[\"median_watch_time_user\"].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a958f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 6.1.6 user dwell / engagement quality features\n",
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"live_id\",\n",
    "    \"streamer_id\",\n",
    "    \"avg_watch_time_user\",\n",
    "    \"median_watch_time_user\",\n",
    "    \"pct_long_watch_user_30s\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "feat_cols = [\n",
    "    \"avg_watch_time_user\",\n",
    "    \"median_watch_time_user\",\n",
    "    \"pct_long_watch_user_30s\",\n",
    "]\n",
    "\n",
    "# missingness\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100)\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "# distribution summary\n",
    "display(df_final_sample[feat_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 6.1.7 User comment behavior (denominator: clicks)\n",
    "df_final_sample = df_final_sample.sort_values([\"user_id\", \"imp_timestamp\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "g = df_final_sample.groupby(\"user_id\", group_keys=False)\n",
    "\n",
    "# past clicks (all history, pre-impression)\n",
    "num_click_user = (\n",
    "    g[\"is_click\"].expanding().sum().shift(1).reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# past comments (all history, pre-impression)\n",
    "num_comment_user = (\n",
    "    g[\"is_comment\"].expanding().sum().shift(1).reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "df_final_sample[\"comment_rate_user\"] = (num_comment_user + 1) / (num_click_user + 1)\n",
    "\n",
    "# 24h window\n",
    "num_comment_user_24h = (\n",
    "    g.rolling(\"24h\", on=\"imp_timestamp\", closed=\"left\")[\"is_comment\"]\n",
    "    .sum()\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "df_final_sample[\"num_comment_user_24h\"] = num_comment_user_24h.to_numpy()\n",
    "df_final_sample[\"has_comment_user_24h\"] = (df_final_sample[\"num_comment_user_24h\"] > 0).astype(\"int64\")\n",
    "df_final_sample[\"num_comment_user_24h\"] = df_final_sample[\"num_comment_user_24h\"].fillna(0)\n",
    "df_final_sample[\"comment_rate_user\"] = df_final_sample[\"comment_rate_user\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0cf189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 6.1.7 user comment behavior features\n",
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"live_id\",\n",
    "    \"streamer_id\",\n",
    "    \"comment_rate_user\",\n",
    "    \"num_comment_user_24h\",\n",
    "    \"has_comment_user_24h\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "feat_cols = [\n",
    "    \"comment_rate_user\",\n",
    "    \"num_comment_user_24h\",\n",
    "    \"has_comment_user_24h\",\n",
    "]\n",
    "\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100)\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "display(df_final_sample[feat_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffdf396",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 6.1.8 User like behavior (denominator: clicks)\n",
    "df_final_sample = df_final_sample.sort_values([\"user_id\", \"imp_timestamp\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "g = df_final_sample.groupby(\"user_id\", group_keys=False)\n",
    "\n",
    "# past clicks (all history, pre-impression)\n",
    "num_click_user = (\n",
    "    g[\"is_click\"].expanding().sum().shift(1).reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# past likes (all history, pre-impression)\n",
    "num_like_user = (\n",
    "    g[\"is_like\"].expanding().sum().shift(1).reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "df_final_sample[\"like_rate_user\"] = (num_like_user + 1) / (num_click_user + 1)\n",
    "\n",
    "# 24h window\n",
    "num_like_user_24h = (\n",
    "    g.rolling(\"24h\", on=\"imp_timestamp\", closed=\"left\")[\"is_like\"]\n",
    "    .sum()\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "df_final_sample[\"num_like_user_24h\"] = num_like_user_24h.to_numpy()\n",
    "df_final_sample[\"has_like_user_24h\"] = (df_final_sample[\"num_like_user_24h\"] > 0).astype(\"int64\")\n",
    "\n",
    "# fill missing\n",
    "df_final_sample[\"num_like_user_24h\"] = df_final_sample[\"num_like_user_24h\"].fillna(0)\n",
    "df_final_sample[\"like_rate_user\"] = df_final_sample[\"like_rate_user\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de681257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 6.1.8 user like behavior features\n",
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"live_id\",\n",
    "    \"streamer_id\",\n",
    "    \"like_rate_user\",\n",
    "    \"num_like_user_24h\",\n",
    "    \"has_like_user_24h\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "feat_cols = [\n",
    "    \"like_rate_user\",\n",
    "    \"num_like_user_24h\",\n",
    "    \"has_like_user_24h\",\n",
    "]\n",
    "\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100)\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "display(df_final_sample[feat_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3562ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 6.1.9 User gift behavior (7d window)\n",
    "\n",
    "df_final_sample[\"imp_timestamp\"] = pd.to_datetime(df_final_sample[\"imp_timestamp\"], errors=\"coerce\")\n",
    "df_final_sample = df_final_sample.sort_values([\"user_id\", \"imp_timestamp\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "g = df_final_sample.groupby(\"user_id\", group_keys=False)\n",
    "\n",
    "# 7d window: gift count + amount\n",
    "num_gift_user_7d = (\n",
    "    g.rolling(\"7d\", on=\"imp_timestamp\", closed=\"left\")[\"is_gift\"]\n",
    "    .sum()\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "amount_gift_user_7d = (\n",
    "    g.rolling(\"7d\", on=\"imp_timestamp\", closed=\"left\")[\"gift_price\"]\n",
    "    .sum()\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "df_final_sample[\"num_gift_user_7d\"] = num_gift_user_7d.to_numpy()\n",
    "df_final_sample[\"amount_gift_user_7d\"] = amount_gift_user_7d.to_numpy()\n",
    "df_final_sample[\"has_gift_user_7d\"] = (df_final_sample[\"num_gift_user_7d\"] > 0).astype(\"int64\")\n",
    "\n",
    "# fill missing\n",
    "df_final_sample[[\"num_gift_user_7d\", \"amount_gift_user_7d\"]] = (\n",
    "    df_final_sample[[\"num_gift_user_7d\", \"amount_gift_user_7d\"]].fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834448a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 6.1.9 user gift behavior features\n",
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"live_id\",\n",
    "    \"streamer_id\",\n",
    "    \"num_gift_user_7d\",\n",
    "    \"amount_gift_user_7d\",\n",
    "    \"has_gift_user_7d\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "feat_cols = [\n",
    "    \"num_gift_user_7d\",\n",
    "    \"amount_gift_user_7d\",\n",
    "    \"has_gift_user_7d\",\n",
    "]\n",
    "\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100)\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "display(df_final_sample[feat_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b963264",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaa0c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feat_6_1 = [\n",
    "    # 6.1.1\n",
    "    \"user_account_age\", \"user_watch_live_age\",\n",
    "    # 6.1.2\n",
    "    \"ctr_user_15min\", \"ctr_user_3hr\", \"ctr_user_1d\", \"ctr_user_7d\",\n",
    "    # 6.1.3\n",
    "    \"num_imp_user_10min\", \"num_imp_user_30min\", \"num_imp_user_2hr\",\n",
    "    \"num_imp_user_12hr\", \"num_imp_user_1d\", \"num_imp_user_7d\",\n",
    "    # 6.1.4\n",
    "    \"num_click_user_15min\", \"num_click_user_3hr\", \"num_click_user_1d\",\n",
    "    \"num_click_user_7d\", \"click_trend_user\",\n",
    "    # 6.1.5\n",
    "    \"time_since_last_impression_user\", \"tsli_missing\",\n",
    "    \"time_since_last_click_user\", \"tslc_missing\",\n",
    "    \"consecutive_skips_user\",\n",
    "    # 6.1.6\n",
    "    \"avg_watch_time_user\", \"avg_watch_time_user_missing\",\n",
    "    \"median_watch_time_user\", \"median_watch_time_user_missing\",  \"pct_long_watch_user_30s\",\n",
    "    # 6.1.7\n",
    "    \"comment_rate_user\", \"has_comment_user_24h\", \"num_comment_user_24h\",\n",
    "    # 6.1.8\n",
    "    \"like_rate_user\", \"has_like_user_24h\", \"num_like_user_24h\",\n",
    "    # 6.1.9\n",
    "    \"has_gift_user_7d\", \"num_gift_user_7d\", \"amount_gift_user_7d\"\n",
    "]\n",
    "\n",
    "print(\"Total engineered features (6.1.16.1.9):\", len(user_feat_6_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc0afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"user_id\", \"streamer_id\", \"live_id\", \"imp_timestamp\"] + user_feat_6_1\n",
    "display(df_final_sample[cols].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f98ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 6.2 Room (Live) Features\n",
    "##### 6.2.1 Basic\n",
    "\t\t\t# time_since_live_start (ms) = imp_timestamp - start_timestamp\n",
    "##### 6.2.2 Room CTR (pre-impression, denominator: impressions)\n",
    "\t\t\t# ctr_room_10min\n",
    "\t\t\t# ctr_room_30min\n",
    "\t\t\t# ctr_room_2hr\n",
    "\t\t\t# ctr_room_12hr\n",
    "##### 6.2.3 Room exposure volume - imp\n",
    "\t\t\t# num_imp_room_10min\n",
    "\t\t\t# num_imp_room_30min\n",
    "\t\t\t# num_imp_room_2hr\n",
    "\t\t\t# num_imp_room_12hr\n",
    "\t\t\t# num_imp_room_1d\n",
    "##### 6.2.4 Room click volume - click\n",
    "\t\t\t# num_click_room_10min\n",
    "\t\t\t# num_click_room_30min\n",
    "\t\t\t# num_click_room_2hr\n",
    "\t\t\t# num_click_room_12hr\n",
    "\t\t\t# num_click_room_1d\n",
    "\t\t\t# ctr_trend_room = log(ctr_room_10min + 1e-6) - log(ctr_room_2hr + 1e-6)\n",
    "##### 6.2.5 Room freshness (leakage-safe)\n",
    "\t\t\t# time_since_start_live\n",
    "\t\t\t# time_since_start_live_bucket: (<5min, 5-20min, >20min)\n",
    "##### 6.2.6 Room dwell / engagement quality (from past clicks only)\n",
    "\t\t\t# avg_watch_time_live\n",
    "\t\t\t# median_watch_time_live\n",
    "\t\t\t# watch_time_live_missing\n",
    "\t\t\t# avg_watch_time_live_30min\n",
    "\t\t\t# median_watch_time_live_30min\n",
    "\t\t\t# watch_time_live_30min_missing\n",
    "\t\t\t# pct_long_watch_live_60s_30min\n",
    "##### 6.2.7 Room comment behavior (Denominator: impressions)\n",
    "\t\t\t# comment_rate_live\n",
    "\t\t\t# comment_rate_live_15min\n",
    "\t\t\t# comment_rate_live_1hr\n",
    "\t\t\t# comment_rate_live_3hr\n",
    "\t\t\t# num_comment_live\n",
    "\t\t\t# num_comment_live_15min\n",
    "\t\t\t# num_comment_live_1hr\n",
    "\t\t\t# num_comment_live_3hr\n",
    "\t\t\t# comment_trend_room = log(comment_rate_live_15min + 1e-6) - log(comment_rate_live_1hr + 1e-6)\n",
    "##### 6.2.8 Room like behavior (Denominator: impressions)\n",
    "\t\t\t# like_rate_live\n",
    "\t\t\t# like_rate_live_15min\n",
    "\t\t\t# like_rate_live_1hr\n",
    "\t\t\t# like_rate_live_3hr\n",
    "\t\t\t# num_like_live\n",
    "\t\t\t# num_like_live_15min\n",
    "\t\t\t# num_like_live_1hr\n",
    "\t\t\t# num_like_live_3hr\n",
    "\t\t\t# like_trend_room = log(like_rate_live_15min + 1e-6) - log(like_rate_live_1hr + 1e-6)\n",
    "##### 6.2.9 Room gift behavior (Denominator: impressions)\n",
    "\t\t\t# gift_rate_live\n",
    "\t\t\t# gift_rate_live_15min\n",
    "\t\t\t# gift_rate_live_1hr\n",
    "\t\t\t# gift_rate_live_3hr\n",
    "\t\t\t# num_gift_live\n",
    "\t\t\t# num_gift_live_15min\n",
    "\t\t\t# num_gift_live_1hr\n",
    "\t\t\t# num_gift_live_3hr\n",
    "\t\t\t# amount_gift_live\n",
    "\t\t\t# amount_gift_live_15min\n",
    "\t\t\t# amount_gift_live_1hr\n",
    "\t\t\t# amount_gift_live_3hr\n",
    "\t\t\t# gift_trend_room = log(log_amount_gift_room_15min + 1) - log(log_amount_gift_room_1hr + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000eb180",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 6.2.1 Basic\n",
    "# time_since_live_start (ms) = imp_timestamp - start_timestamp\n",
    "df_final_sample[\"time_since_live_start\"] = (\n",
    "    (df_final_sample[\"imp_timestamp\"] - df_final_sample[\"start_timestamp\"])\n",
    "    .dt.total_seconds()\n",
    "    .mul(1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aa0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 6.2.1 time_since_live_start\n",
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"live_id\",\n",
    "    \"streamer_id\",\n",
    "    \"imp_timestamp\",\n",
    "    \"start_timestamp\",\n",
    "    \"time_since_live_start\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "feat_cols = [\"time_since_live_start\"]\n",
    "\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100)\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "display(df_final_sample[feat_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 6.2.2 Room CTR (pre-impression, denominator: impressions)\n",
    "df_final_sample = df_final_sample.sort_values([\"live_id\", \"imp_timestamp\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "g = df_final_sample.groupby(\"live_id\", group_keys=False)\n",
    "\n",
    "room_windows = {\n",
    "    \"10min\": \"ctr_room_10min\",\n",
    "    \"30min\": \"ctr_room_30min\",\n",
    "    \"2h\": \"ctr_room_2hr\",\n",
    "    \"12h\": \"ctr_room_12hr\",\n",
    "}\n",
    "\n",
    "for w, col in room_windows.items():\n",
    "    clicks = (\n",
    "        g.rolling(w, on=\"imp_timestamp\", closed=\"left\")[\"is_click\"]\n",
    "        .sum()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    imps = (\n",
    "        g.rolling(w, on=\"imp_timestamp\", closed=\"left\")[\"is_click\"]\n",
    "        .count()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    df_final_sample[col] = (clicks / imps.replace(0, np.nan)).fillna(0).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da321899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 6.2.2 room CTR features\n",
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"live_id\",\n",
    "    \"streamer_id\",\n",
    "    \"ctr_room_10min\",\n",
    "    \"ctr_room_30min\",\n",
    "    \"ctr_room_2hr\",\n",
    "    \"ctr_room_12hr\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "feat_cols = [\n",
    "    \"ctr_room_10min\",\n",
    "    \"ctr_room_30min\",\n",
    "    \"ctr_room_2hr\",\n",
    "    \"ctr_room_12hr\",\n",
    "]\n",
    "\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100)\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "display(df_final_sample[feat_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27b49ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 6.2.3 Room exposure volume - imp\n",
    "df_final_sample = df_final_sample.sort_values([\"live_id\", \"imp_timestamp\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "g = df_final_sample.groupby(\"live_id\", group_keys=False)\n",
    "\n",
    "room_imp_windows = {\n",
    "    \"10min\": \"num_imp_room_10min\",\n",
    "    \"30min\": \"num_imp_room_30min\",\n",
    "    \"2h\": \"num_imp_room_2hr\",\n",
    "    \"12h\": \"num_imp_room_12hr\",\n",
    "    \"1d\": \"num_imp_room_1d\",\n",
    "}\n",
    "\n",
    "for w, col in room_imp_windows.items():\n",
    "    imps = (\n",
    "        g.rolling(w, on=\"imp_timestamp\", closed=\"left\")[\"is_click\"]\n",
    "        .count()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    df_final_sample[col] = imps.to_numpy()\n",
    "\n",
    "\n",
    "# for these newly added features, replace missing values with 0\n",
    "room_imp_cols = [\n",
    "    \"num_imp_room_10min\",\n",
    "    \"num_imp_room_30min\",\n",
    "    \"num_imp_room_2hr\",\n",
    "    \"num_imp_room_12hr\",\n",
    "    \"num_imp_room_1d\"\n",
    "]\n",
    "\n",
    "df_final_sample[room_imp_cols] = df_final_sample[room_imp_cols].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b1e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 6.2.3 room exposure volume features\n",
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"live_id\",\n",
    "    \"streamer_id\",\n",
    "    \"num_imp_room_10min\",\n",
    "    \"num_imp_room_30min\",\n",
    "    \"num_imp_room_2hr\",\n",
    "    \"num_imp_room_12hr\",\n",
    "    \"num_imp_room_1d\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "feat_cols = [\n",
    "    \"num_imp_room_10min\",\n",
    "    \"num_imp_room_30min\",\n",
    "    \"num_imp_room_2hr\",\n",
    "    \"num_imp_room_12hr\",\n",
    "    \"num_imp_room_1d\",\n",
    "]\n",
    "\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100)\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "display(df_final_sample[feat_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653ee1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 6.2.4 Room click volume - click\n",
    "df_final_sample = df_final_sample.sort_values([\"live_id\", \"imp_timestamp\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "g = df_final_sample.groupby(\"live_id\", group_keys=False)\n",
    "\n",
    "room_click_windows = {\n",
    "    \"10min\": \"num_click_room_10min\",\n",
    "    \"30min\": \"num_click_room_30min\",\n",
    "    \"2h\": \"num_click_room_2hr\",\n",
    "    \"12h\": \"num_click_room_12hr\",\n",
    "    \"1d\": \"num_click_room_1d\",\n",
    "}\n",
    "\n",
    "for w, col in room_click_windows.items():\n",
    "    clicks = (\n",
    "        g.rolling(w, on=\"imp_timestamp\", closed=\"left\")[\"is_click\"]\n",
    "        .sum()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    df_final_sample[col] = clicks.to_numpy()\n",
    "\n",
    "# ctr trend\n",
    "df_final_sample[\"ctr_trend_room\"] = (\n",
    "    np.log(df_final_sample[\"ctr_room_10min\"] + 1e-6)\n",
    "    - np.log(df_final_sample[\"ctr_room_2hr\"] + 1e-6)\n",
    ")\n",
    "\n",
    "# fill missing\n",
    "df_final_sample[list(room_click_windows.values()) + [\"ctr_trend_room\"]] = (\n",
    "    df_final_sample[list(room_click_windows.values()) + [\"ctr_trend_room\"]].fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02948b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 6.2.4 room click volume features\n",
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"live_id\",\n",
    "    \"streamer_id\",\n",
    "    \"num_click_room_10min\",\n",
    "    \"num_click_room_30min\",\n",
    "    \"num_click_room_2hr\",\n",
    "    \"num_click_room_12hr\",\n",
    "    \"num_click_room_1d\",\n",
    "    \"ctr_trend_room\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "feat_cols = [\n",
    "    \"num_click_room_10min\",\n",
    "    \"num_click_room_30min\",\n",
    "    \"num_click_room_2hr\",\n",
    "    \"num_click_room_12hr\",\n",
    "    \"num_click_room_1d\",\n",
    "    \"ctr_trend_room\",\n",
    "]\n",
    "\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100)\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "display(df_final_sample[feat_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bf3f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 6.2.5 Room freshness (leakage-safe)\n",
    "df_final_sample[\"start_timestamp\"] = pd.to_datetime(df_final_sample[\"start_timestamp\"], errors=\"coerce\")\n",
    "\n",
    "# time since live start (ms)\n",
    "df_final_sample[\"time_since_start_live\"] = (\n",
    "    (df_final_sample[\"imp_timestamp\"] - df_final_sample[\"start_timestamp\"])\n",
    "    .dt.total_seconds()\n",
    "    .mul(1000)\n",
    ")\n",
    "\n",
    "# buckets based on minutes: <5min, 5-20min, >20min\n",
    "mins = df_final_sample[\"time_since_start_live\"] / (60 * 1000)\n",
    "df_final_sample[\"time_since_start_live_bucket\"] = pd.cut(\n",
    "    mins,\n",
    "    bins=[-float(\"inf\"), 5, 20, float(\"inf\")],\n",
    "    labels=[\"<5min\", \"5-20min\", \">20min\"],\n",
    ")\n",
    "# label encoding for time_since_start_live_bucket\n",
    "df_final_sample[\"time_since_start_live_bucket\"] = (\n",
    "    df_final_sample[\"time_since_start_live_bucket\"].astype(\"category\").cat.codes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ee667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 6.2.5 room freshness features\n",
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"live_id\",\n",
    "    \"streamer_id\",\n",
    "    \"time_since_start_live\",\n",
    "    \"time_since_start_live_bucket\"\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "feat_cols = [\"time_since_start_live\"]\n",
    "\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100)\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "# bucket distribution\n",
    "display(df_final_sample[\"time_since_start_live_bucket\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ece8f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 6.2.6 Room dwell / engagement quality (from past clicks only)\n",
    "df_final_sample = df_final_sample.sort_values([\"live_id\", \"imp_timestamp\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "# watch time only on clicks\n",
    "df_final_sample[\"_watch_on_click\"] = df_final_sample[\"watch_live_time\"].where(df_final_sample[\"is_click\"] == 1)\n",
    "\n",
    "g = df_final_sample.groupby(\"live_id\", group_keys=False)\n",
    "\n",
    "# all-history (past only)\n",
    "df_final_sample[\"avg_watch_time_live\"] = (\n",
    "    g[\"_watch_on_click\"].expanding().mean().shift(1).reset_index(level=0, drop=True)\n",
    ")\n",
    "df_final_sample[\"median_watch_time_live\"] = (\n",
    "    g[\"_watch_on_click\"].expanding().median().shift(1).reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# 30min rolling (past only)\n",
    "df_final_sample[\"avg_watch_time_live_30min\"] = (\n",
    "    g.rolling(\"30min\", on=\"imp_timestamp\", closed=\"left\")[\"_watch_on_click\"]\n",
    "    .mean()\n",
    "    .reset_index(level=0, drop=True)\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "df_final_sample[\"median_watch_time_live_30min\"] = (\n",
    "    g.rolling(\"30min\", on=\"imp_timestamp\", closed=\"left\")[\"_watch_on_click\"]\n",
    "    .median()\n",
    "    .reset_index(level=0, drop=True)\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "\n",
    "# pct_long_watch_live_60s_30min (past only)\n",
    "past_long_30 = (\n",
    "    g.rolling(\"30min\", on=\"imp_timestamp\", closed=\"left\")[\"_watch_on_click\"]\n",
    "    .apply(lambda s: (s >= 60000).sum(), raw=True)\n",
    "    .reset_index(level=0, drop=True)\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "past_clicks_30 = (\n",
    "    g.rolling(\"30min\", on=\"imp_timestamp\", closed=\"left\")[\"_watch_on_click\"]\n",
    "    .count()\n",
    "    .reset_index(level=0, drop=True)\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "df_final_sample[\"pct_long_watch_live_60s_30min\"] = (\n",
    "    past_long_30 / np.where(past_clicks_30 == 0, np.nan, past_clicks_30)\n",
    ")\n",
    "df_final_sample[\"pct_long_watch_live_60s_30min\"] = df_final_sample[\"pct_long_watch_live_60s_30min\"].fillna(0)\n",
    "\n",
    "\n",
    "# missing flags for watch-time stats (avg/median share one flag per window)\n",
    "df_final_sample[\"watch_time_live_missing\"] = (\n",
    "    df_final_sample[[\"avg_watch_time_live\", \"median_watch_time_live\"]].isna().any(axis=1).astype(\"int64\")\n",
    ")\n",
    "df_final_sample[\"watch_time_live_30min_missing\"] = (\n",
    "    df_final_sample[[\"avg_watch_time_live_30min\", \"median_watch_time_live_30min\"]].isna().any(axis=1).astype(\"int64\")\n",
    ")\n",
    "\n",
    "\n",
    "# fill NaNs with 0\n",
    "fill_cols = [\n",
    "    \"avg_watch_time_live\",\n",
    "    \"median_watch_time_live\",\n",
    "    \"avg_watch_time_live_30min\",\n",
    "    \"median_watch_time_live_30min\",\n",
    "]\n",
    "df_final_sample[fill_cols] = df_final_sample[fill_cols].fillna(0)\n",
    "\n",
    "# cleanup helper\n",
    "df_final_sample = df_final_sample.drop(columns=[\"_watch_on_click\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e2072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 6.2.6 room dwell / engagement quality features\n",
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"live_id\",\n",
    "    \"streamer_id\",\n",
    "    \"avg_watch_time_live\",\n",
    "    \"median_watch_time_live\",\n",
    "    \"avg_watch_time_live_30min\",\n",
    "    \"median_watch_time_live_30min\",\n",
    "    \"pct_long_watch_live_60s_30min\",\n",
    "    \"watch_time_live_missing\",\n",
    "    \"watch_time_live_30min_missing\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "feat_cols = [\n",
    "    \"avg_watch_time_live\",\n",
    "    \"median_watch_time_live\",\n",
    "    \"avg_watch_time_live_30min\",\n",
    "    \"median_watch_time_live_30min\",\n",
    "    \"pct_long_watch_live_60s_30min\",\n",
    "    \"watch_time_live_missing\",\n",
    "    \"watch_time_live_30min_missing\",\n",
    "]\n",
    "\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100)\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "display(df_final_sample[feat_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae176df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 6.2.7 Room comment behavior (denominator: impressions)\n",
    "df_final_sample = df_final_sample.sort_values([\"live_id\", \"imp_timestamp\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "g = df_final_sample.groupby(\"live_id\", group_keys=False)\n",
    "\n",
    "# all-history (past only)\n",
    "num_comment_live = (\n",
    "    g[\"is_comment\"].expanding().sum().shift(1).reset_index(level=0, drop=True)\n",
    ")\n",
    "num_imp_live = (\n",
    "    g[\"is_comment\"].expanding().count().shift(1).reset_index(level=0, drop=True)\n",
    ")\n",
    "df_final_sample[\"num_comment_live\"] = num_comment_live.to_numpy()\n",
    "df_final_sample[\"comment_rate_live\"] = (num_comment_live / num_imp_live.replace(0, np.nan)).fillna(0)\n",
    "\n",
    "# rolling windows (past only)\n",
    "def _roll_sum(w):\n",
    "    return (\n",
    "        g.rolling(w, on=\"imp_timestamp\", closed=\"left\")[\"is_comment\"]\n",
    "        .sum()\n",
    "        .reset_index(level=0, drop=True)\n",
    "        .to_numpy()\n",
    "    )\n",
    "\n",
    "def _roll_cnt(w):\n",
    "    return (\n",
    "        g.rolling(w, on=\"imp_timestamp\", closed=\"left\")[\"is_comment\"]\n",
    "        .count()\n",
    "        .reset_index(level=0, drop=True)\n",
    "        .to_numpy()\n",
    "    )\n",
    "\n",
    "df_final_sample[\"num_comment_live_15min\"] = _roll_sum(\"15min\")\n",
    "df_final_sample[\"num_comment_live_1hr\"]  = _roll_sum(\"1h\")\n",
    "df_final_sample[\"num_comment_live_3hr\"]  = _roll_sum(\"3h\")\n",
    "\n",
    "imp_15 = _roll_cnt(\"15min\")\n",
    "imp_1h = _roll_cnt(\"1h\")\n",
    "imp_3h = _roll_cnt(\"3h\")\n",
    "\n",
    "df_final_sample[\"comment_rate_live_15min\"] = (df_final_sample[\"num_comment_live_15min\"] / np.where(imp_15 == 0, np.nan, imp_15))\n",
    "df_final_sample[\"comment_rate_live_1hr\"]   = (df_final_sample[\"num_comment_live_1hr\"]  / np.where(imp_1h == 0, np.nan, imp_1h))\n",
    "df_final_sample[\"comment_rate_live_3hr\"]   = (df_final_sample[\"num_comment_live_3hr\"]  / np.where(imp_3h == 0, np.nan, imp_3h))\n",
    "\n",
    "# trend\n",
    "df_final_sample[\"comment_trend_room\"] = (\n",
    "    np.log(df_final_sample[\"comment_rate_live_15min\"] + 1e-6)\n",
    "    - np.log(df_final_sample[\"comment_rate_live_1hr\"] + 1e-6)\n",
    ")\n",
    "\n",
    "# fill missing\n",
    "fill_cols = [\n",
    "    \"num_comment_live\", \"num_comment_live_15min\", \"num_comment_live_1hr\", \"num_comment_live_3hr\",\n",
    "    \"comment_rate_live\", \"comment_rate_live_15min\", \"comment_rate_live_1hr\", \"comment_rate_live_3hr\",\n",
    "    \"comment_trend_room\",\n",
    "]\n",
    "df_final_sample[fill_cols] = df_final_sample[fill_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1245c8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 6.2.7 room comment behavior features\n",
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"live_id\",\n",
    "    \"streamer_id\",\n",
    "    \"num_comment_live\",\n",
    "    \"num_comment_live_15min\",\n",
    "    \"num_comment_live_1hr\",\n",
    "    \"num_comment_live_3hr\",\n",
    "    \"comment_rate_live\",\n",
    "    \"comment_rate_live_15min\",\n",
    "    \"comment_rate_live_1hr\",\n",
    "    \"comment_rate_live_3hr\",\n",
    "    \"comment_trend_room\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "feat_cols = [\n",
    "    \"num_comment_live\",\n",
    "    \"num_comment_live_15min\",\n",
    "    \"num_comment_live_1hr\",\n",
    "    \"num_comment_live_3hr\",\n",
    "    \"comment_rate_live\",\n",
    "    \"comment_rate_live_15min\",\n",
    "    \"comment_rate_live_1hr\",\n",
    "    \"comment_rate_live_3hr\",\n",
    "    \"comment_trend_room\",\n",
    "]\n",
    "\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100)\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "display(df_final_sample[feat_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31081c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 6.2.8 Room like behavior (denominator: impressions)\n",
    "df_final_sample = df_final_sample.sort_values([\"live_id\", \"imp_timestamp\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "g = df_final_sample.groupby(\"live_id\", group_keys=False)\n",
    "\n",
    "# all-history (past only)\n",
    "num_like_live = (\n",
    "    g[\"is_like\"].expanding().sum().shift(1).reset_index(level=0, drop=True)\n",
    ")\n",
    "num_imp_live = (\n",
    "    g[\"is_like\"].expanding().count().shift(1).reset_index(level=0, drop=True)\n",
    ")\n",
    "df_final_sample[\"num_like_live\"] = num_like_live.to_numpy()\n",
    "df_final_sample[\"like_rate_live\"] = (num_like_live / num_imp_live.replace(0, np.nan)).fillna(0)\n",
    "\n",
    "# rolling windows (past only)\n",
    "def _roll_sum(w):\n",
    "    return (\n",
    "        g.rolling(w, on=\"imp_timestamp\", closed=\"left\")[\"is_like\"]\n",
    "        .sum()\n",
    "        .reset_index(level=0, drop=True)\n",
    "        .to_numpy()\n",
    "    )\n",
    "\n",
    "def _roll_cnt(w):\n",
    "    return (\n",
    "        g.rolling(w, on=\"imp_timestamp\", closed=\"left\")[\"is_like\"]\n",
    "        .count()\n",
    "        .reset_index(level=0, drop=True)\n",
    "        .to_numpy()\n",
    "    )\n",
    "\n",
    "df_final_sample[\"num_like_live_15min\"] = _roll_sum(\"15min\")\n",
    "df_final_sample[\"num_like_live_1hr\"]  = _roll_sum(\"1h\")\n",
    "df_final_sample[\"num_like_live_3hr\"]  = _roll_sum(\"3h\")\n",
    "\n",
    "imp_15 = _roll_cnt(\"15min\")\n",
    "imp_1h = _roll_cnt(\"1h\")\n",
    "imp_3h = _roll_cnt(\"3h\")\n",
    "\n",
    "df_final_sample[\"like_rate_live_15min\"] = (df_final_sample[\"num_like_live_15min\"] / np.where(imp_15 == 0, np.nan, imp_15))\n",
    "df_final_sample[\"like_rate_live_1hr\"]   = (df_final_sample[\"num_like_live_1hr\"]  / np.where(imp_1h == 0, np.nan, imp_1h))\n",
    "df_final_sample[\"like_rate_live_3hr\"]   = (df_final_sample[\"num_like_live_3hr\"]  / np.where(imp_3h == 0, np.nan, imp_3h))\n",
    "\n",
    "# trend\n",
    "df_final_sample[\"like_trend_room\"] = (\n",
    "    np.log(df_final_sample[\"like_rate_live_15min\"] + 1e-6)\n",
    "    - np.log(df_final_sample[\"like_rate_live_1hr\"] + 1e-6)\n",
    ")\n",
    "\n",
    "# fill missing\n",
    "fill_cols = [\n",
    "    \"num_like_live\", \"num_like_live_15min\", \"num_like_live_1hr\", \"num_like_live_3hr\",\n",
    "    \"like_rate_live\", \"like_rate_live_15min\", \"like_rate_live_1hr\", \"like_rate_live_3hr\",\n",
    "    \"like_trend_room\",\n",
    "]\n",
    "df_final_sample[fill_cols] = df_final_sample[fill_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c722f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 6.2.8 room like behavior features\n",
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"live_id\",\n",
    "    \"streamer_id\",\n",
    "    \"num_like_live\",\n",
    "    \"num_like_live_15min\",\n",
    "    \"num_like_live_1hr\",\n",
    "    \"num_like_live_3hr\",\n",
    "    \"like_rate_live\",\n",
    "    \"like_rate_live_15min\",\n",
    "    \"like_rate_live_1hr\",\n",
    "    \"like_rate_live_3hr\",\n",
    "    \"like_trend_room\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "feat_cols = [\n",
    "    \"num_like_live\",\n",
    "    \"num_like_live_15min\",\n",
    "    \"num_like_live_1hr\",\n",
    "    \"num_like_live_3hr\",\n",
    "    \"like_rate_live\",\n",
    "    \"like_rate_live_15min\",\n",
    "    \"like_rate_live_1hr\",\n",
    "    \"like_rate_live_3hr\",\n",
    "    \"like_trend_room\",\n",
    "]\n",
    "\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100)\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "display(df_final_sample[feat_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11215f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 6.2.9 Room gift behavior (denominator: impressions)\n",
    "df_final_sample = df_final_sample.sort_values([\"live_id\", \"imp_timestamp\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "g = df_final_sample.groupby(\"live_id\", group_keys=False)\n",
    "\n",
    "# all-history (past only)\n",
    "num_gift_live = (\n",
    "    g[\"is_gift\"].expanding().sum().shift(1).reset_index(level=0, drop=True)\n",
    ")\n",
    "amount_gift_live = (\n",
    "    g[\"gift_price\"].expanding().sum().shift(1).reset_index(level=0, drop=True)\n",
    ")\n",
    "num_imp_live = (\n",
    "    g[\"is_gift\"].expanding().count().shift(1).reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "df_final_sample[\"num_gift_live\"] = num_gift_live.to_numpy()\n",
    "df_final_sample[\"amount_gift_live\"] = amount_gift_live.to_numpy()\n",
    "df_final_sample[\"gift_rate_live\"] = (num_gift_live / num_imp_live.replace(0, np.nan)).fillna(0)\n",
    "\n",
    "# rolling windows (past only)\n",
    "def _roll_sum(col, w):\n",
    "    return (\n",
    "        g.rolling(w, on=\"imp_timestamp\", closed=\"left\")[col]\n",
    "        .sum()\n",
    "        .reset_index(level=0, drop=True)\n",
    "        .to_numpy()\n",
    "    )\n",
    "\n",
    "def _roll_cnt(w):\n",
    "    return (\n",
    "        g.rolling(w, on=\"imp_timestamp\", closed=\"left\")[\"is_gift\"]\n",
    "        .count()\n",
    "        .reset_index(level=0, drop=True)\n",
    "        .to_numpy()\n",
    "    )\n",
    "\n",
    "df_final_sample[\"num_gift_live_15min\"] = _roll_sum(\"is_gift\", \"15min\")\n",
    "df_final_sample[\"num_gift_live_1hr\"]  = _roll_sum(\"is_gift\", \"1h\")\n",
    "df_final_sample[\"num_gift_live_3hr\"]  = _roll_sum(\"is_gift\", \"3h\")\n",
    "\n",
    "df_final_sample[\"amount_gift_live_15min\"] = _roll_sum(\"gift_price\", \"15min\")\n",
    "df_final_sample[\"amount_gift_live_1hr\"]  = _roll_sum(\"gift_price\", \"1h\")\n",
    "df_final_sample[\"amount_gift_live_3hr\"]  = _roll_sum(\"gift_price\", \"3h\")\n",
    "\n",
    "imp_15 = _roll_cnt(\"15min\")\n",
    "imp_1h = _roll_cnt(\"1h\")\n",
    "imp_3h = _roll_cnt(\"3h\")\n",
    "\n",
    "df_final_sample[\"gift_rate_live_15min\"] = (df_final_sample[\"num_gift_live_15min\"] / np.where(imp_15 == 0, np.nan, imp_15))\n",
    "df_final_sample[\"gift_rate_live_1hr\"]   = (df_final_sample[\"num_gift_live_1hr\"]  / np.where(imp_1h == 0, np.nan, imp_1h))\n",
    "df_final_sample[\"gift_rate_live_3hr\"]   = (df_final_sample[\"num_gift_live_3hr\"]  / np.where(imp_3h == 0, np.nan, imp_3h))\n",
    "\n",
    "# trend (log amount)\n",
    "df_final_sample[\"gift_trend_room\"] = (\n",
    "    np.log(np.log1p(df_final_sample[\"amount_gift_live_15min\"]) + 1)\n",
    "    - np.log(np.log1p(df_final_sample[\"amount_gift_live_1hr\"]) + 1)\n",
    ")\n",
    "\n",
    "# fill missing\n",
    "fill_cols = [\n",
    "    \"num_gift_live\", \"amount_gift_live\", \"gift_rate_live\",\n",
    "    \"num_gift_live_15min\", \"num_gift_live_1hr\", \"num_gift_live_3hr\",\n",
    "    \"amount_gift_live_15min\", \"amount_gift_live_1hr\", \"amount_gift_live_3hr\",\n",
    "    \"gift_rate_live_15min\", \"gift_rate_live_1hr\", \"gift_rate_live_3hr\",\n",
    "    \"gift_trend_room\",\n",
    "]\n",
    "df_final_sample[fill_cols] = df_final_sample[fill_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b869d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 6.2.9 room gift behavior features\n",
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"live_id\",\n",
    "    \"streamer_id\",\n",
    "    \"num_gift_live\",\n",
    "    \"amount_gift_live\",\n",
    "    \"gift_rate_live\",\n",
    "    \"num_gift_live_15min\",\n",
    "    \"num_gift_live_1hr\",\n",
    "    \"num_gift_live_3hr\",\n",
    "    \"amount_gift_live_15min\",\n",
    "    \"amount_gift_live_1hr\",\n",
    "    \"amount_gift_live_3hr\",\n",
    "    \"gift_rate_live_15min\",\n",
    "    \"gift_rate_live_1hr\",\n",
    "    \"gift_rate_live_3hr\",\n",
    "    \"gift_trend_room\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "feat_cols = [\n",
    "    \"num_gift_live\",\n",
    "    \"amount_gift_live\",\n",
    "    \"gift_rate_live\",\n",
    "    \"num_gift_live_15min\",\n",
    "    \"num_gift_live_1hr\",\n",
    "    \"num_gift_live_3hr\",\n",
    "    \"amount_gift_live_15min\",\n",
    "    \"amount_gift_live_1hr\",\n",
    "    \"amount_gift_live_3hr\",\n",
    "    \"gift_rate_live_15min\",\n",
    "    \"gift_rate_live_1hr\",\n",
    "    \"gift_rate_live_3hr\",\n",
    "    \"gift_trend_room\",\n",
    "]\n",
    "\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100)\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "display(df_final_sample[feat_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c974024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "room_feat_6_2 = [\n",
    "    # 6.2.1\n",
    "    \"time_since_live_start\",\n",
    "    # 6.2.2\n",
    "    \"ctr_room_10min\", \"ctr_room_30min\", \"ctr_room_2hr\", \"ctr_room_12hr\",\n",
    "    # 6.2.3\n",
    "    \"num_imp_room_10min\", \"num_imp_room_30min\", \"num_imp_room_2hr\",\n",
    "    \"num_imp_room_12hr\", \"num_imp_room_1d\",\n",
    "    # 6.2.4\n",
    "    \"num_click_room_10min\", \"num_click_room_30min\", \"num_click_room_2hr\",\n",
    "    \"num_click_room_12hr\", \"num_click_room_1d\", \"ctr_trend_room\",\n",
    "    # 6.2.5\n",
    "    \"time_since_start_live\", \"time_since_start_live_bucket\",\n",
    "    # 6.2.6\n",
    "    \"avg_watch_time_live\", \"median_watch_time_live\",\n",
    "    \"avg_watch_time_live_30min\", \"median_watch_time_live_30min\",\n",
    "    \"pct_long_watch_live_60s_30min\",\n",
    "    \"watch_time_live_missing\", \"watch_time_live_30min_missing\",\n",
    "    # 6.2.7\n",
    "    \"comment_rate_live\", \"comment_rate_live_15min\", \"comment_rate_live_1hr\", \"comment_rate_live_3hr\",\n",
    "    \"num_comment_live\", \"num_comment_live_15min\", \"num_comment_live_1hr\", \"num_comment_live_3hr\",\n",
    "    \"comment_trend_room\",\n",
    "    # 6.2.8\n",
    "    \"like_rate_live\", \"like_rate_live_15min\", \"like_rate_live_1hr\", \"like_rate_live_3hr\",\n",
    "    \"num_like_live\", \"num_like_live_15min\", \"num_like_live_1hr\", \"num_like_live_3hr\",\n",
    "    \"like_trend_room\",\n",
    "    # 6.2.9\n",
    "    \"gift_rate_live\", \"gift_rate_live_15min\", \"gift_rate_live_1hr\", \"gift_rate_live_3hr\",\n",
    "    \"num_gift_live\", \"num_gift_live_15min\", \"num_gift_live_1hr\", \"num_gift_live_3hr\",\n",
    "    \"amount_gift_live\", \"amount_gift_live_15min\", \"amount_gift_live_1hr\", \"amount_gift_live_3hr\",\n",
    "    \"gift_trend_room\",\n",
    "]\n",
    "\n",
    "print(\"Total engineered room features (6.2.16.2.9):\", len(room_feat_6_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71500883",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"user_id\", \"streamer_id\", \"live_id\", \"imp_timestamp\"] + room_feat_6_2\n",
    "\n",
    "with pd.option_context(\"display.max_columns\", None, \"display.width\", None):\n",
    "    display(df_final_sample[cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb01fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa94ccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 6.3 Streamer Features\n",
    "##### 6.3.1 Basic\n",
    "\t\t\t# streamer_account_age = imp_timestamp - streamer_reg_timestamp\n",
    "\t\t\t# streamer_live_age = imp_timestamp - first_live_timestamp\n",
    "##### 6.3.2 Streamer CTR / volume\n",
    "\t\t\t# ctr_streamer_1d\n",
    "\t\t\t# ctr_streamer_7d\n",
    "\t\t\t# num_imp_streamer_7d\n",
    "\t\t\t# num_click_streamer_7d\n",
    "\t\t\t# num_lives_streamer_7d\n",
    "##### 6.3.3 Streamer engagement quality - dwell time (from past clicks only)\n",
    "\t\t\t# avg_watch_time_streamer\n",
    "\t\t\t# median_watch_time_streamer\n",
    "\t\t\t# pct_long_watch_streamer_30s\n",
    "##### 6.3.4 Streamer interaction volume\n",
    "\t\t\t# num_comment_streamer_7d\n",
    "\t\t\t# num_like_streamer_7d\n",
    "\t\t\t# amount_gift_streamer_7d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853681fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3.1 Streamer basic features\n",
    "# streamer_account_age (days) = imp_timestamp - streamer_reg_timestamp\n",
    "# streamer_live_age (days) = imp_timestamp - first_live_timestamp\n",
    "\n",
    "# compute ages (in days)\n",
    "df_final_sample[\"streamer_account_age\"] = (\n",
    "    (df_final_sample[\"imp_timestamp\"] - df_final_sample[\"streamer_reg_timestamp\"])\n",
    "    .dt.total_seconds() / 86400\n",
    ")\n",
    "df_final_sample[\"streamer_live_age\"] = (\n",
    "    (df_final_sample[\"imp_timestamp\"] - df_final_sample[\"first_live_timestamp\"])\n",
    "    .dt.total_seconds() / 86400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed8451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks for 6.3.1 streamer basic features\n",
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"live_id\",\n",
    "    \"streamer_id\",\n",
    "    \"imp_timestamp\",\n",
    "    \"streamer_reg_timestamp\",\n",
    "    \"first_live_timestamp\",\n",
    "    \"streamer_account_age\",\n",
    "    \"streamer_live_age\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "feat_cols = [\n",
    "    \"streamer_account_age\",\n",
    "    \"streamer_live_age\",\n",
    "]\n",
    "\n",
    "# missingness\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100),\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "# distribution summary\n",
    "display(df_final_sample[feat_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))\n",
    "\n",
    "# sanity checks: negative ages\n",
    "neg_account = (df_final_sample[\"streamer_account_age\"] < 0).sum()\n",
    "neg_live = (df_final_sample[\"streamer_live_age\"] < 0).sum()\n",
    "\n",
    "print(\"negative streamer_account_age:\", int(neg_account))\n",
    "print(\"negative streamer_live_age:\", int(neg_live))\n",
    "\n",
    "# show a few problematic rows if any\n",
    "if neg_account > 0 or neg_live > 0:\n",
    "    bad = df_final_sample[\n",
    "        (df_final_sample[\"streamer_account_age\"] < 0) |\n",
    "        (df_final_sample[\"streamer_live_age\"] < 0)\n",
    "    ][cols]\n",
    "    display(bad.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663843bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3.2 Streamer CTR / volume (past-only, leakage-safe)\n",
    "df_final_sample = df_final_sample.sort_values([\"streamer_id\", \"imp_timestamp\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "g = df_final_sample.groupby(\"streamer_id\", group_keys=False)\n",
    "\n",
    "# helper: rolling counts with time windows (past only)\n",
    "def _roll_cnt(w):\n",
    "    return (\n",
    "        g.rolling(w, on=\"imp_timestamp\", closed=\"left\")[\"is_click\"]\n",
    "        .count()\n",
    "        .reset_index(level=0, drop=True)\n",
    "        .to_numpy()\n",
    "    )\n",
    "\n",
    "def _roll_sum(w):\n",
    "    return (\n",
    "        g.rolling(w, on=\"imp_timestamp\", closed=\"left\")[\"is_click\"]\n",
    "        .sum()\n",
    "        .reset_index(level=0, drop=True)\n",
    "        .to_numpy()\n",
    "    )\n",
    "\n",
    "# 7d window\n",
    "imp_7d = _roll_cnt(\"7d\")\n",
    "click_7d = _roll_sum(\"7d\")\n",
    "\n",
    "df_final_sample[\"num_imp_streamer_7d\"] = imp_7d\n",
    "df_final_sample[\"num_click_streamer_7d\"] = click_7d\n",
    "df_final_sample[\"ctr_streamer_7d\"] = (click_7d / np.where(imp_7d == 0, np.nan, imp_7d))\n",
    "\n",
    "# 1d window\n",
    "imp_1d = _roll_cnt(\"1d\")\n",
    "click_1d = _roll_sum(\"1d\")\n",
    "\n",
    "df_final_sample[\"ctr_streamer_1d\"] = (click_1d / np.where(imp_1d == 0, np.nan, imp_1d))\n",
    "\n",
    "# num_lives_streamer_7d: unique live_id in past 7d\n",
    "df_final_sample[\"num_lives_streamer_7d\"] = (\n",
    "    g.rolling(\"7d\", on=\"imp_timestamp\", closed=\"left\")[\"live_id\"]\n",
    "    .apply(lambda s: s.nunique(), raw=False)\n",
    "    .reset_index(level=0, drop=True)\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "# fill missing with 0\n",
    "fill_cols = [\n",
    "    \"num_imp_streamer_7d\",\n",
    "    \"num_click_streamer_7d\",\n",
    "    \"ctr_streamer_7d\",\n",
    "    \"ctr_streamer_1d\",\n",
    "    \"num_lives_streamer_7d\",\n",
    "]\n",
    "df_final_sample[fill_cols] = df_final_sample[fill_cols].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7735d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 6.3.2 streamer CTR / volume features\n",
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"live_id\",\n",
    "    \"streamer_id\",\n",
    "    \"imp_timestamp\",\n",
    "    \"num_imp_streamer_7d\",\n",
    "    \"num_click_streamer_7d\",\n",
    "    \"ctr_streamer_7d\",\n",
    "    \"ctr_streamer_1d\",\n",
    "    \"num_lives_streamer_7d\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "feat_cols = [\n",
    "    \"num_imp_streamer_7d\",\n",
    "    \"num_click_streamer_7d\",\n",
    "    \"ctr_streamer_7d\",\n",
    "    \"ctr_streamer_1d\",\n",
    "    \"num_lives_streamer_7d\",\n",
    "]\n",
    "\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100),\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "display(df_final_sample[feat_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58701b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3.3 Streamer engagement quality - dwell time (past clicks only)\n",
    "df_final_sample = df_final_sample.sort_values([\"streamer_id\", \"imp_timestamp\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "# watch time only on clicks\n",
    "df_final_sample[\"_watch_on_click\"] = df_final_sample[\"watch_live_time\"].where(df_final_sample[\"is_click\"] == 1)\n",
    "\n",
    "g = df_final_sample.groupby(\"streamer_id\", group_keys=False)\n",
    "\n",
    "# all-history (past only)\n",
    "df_final_sample[\"avg_watch_time_streamer\"] = (\n",
    "    g[\"_watch_on_click\"].expanding().mean().shift(1).reset_index(level=0, drop=True)\n",
    ")\n",
    "df_final_sample[\"median_watch_time_streamer\"] = (\n",
    "    g[\"_watch_on_click\"].expanding().median().shift(1).reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# pct_long_watch_streamer_30s (past only)\n",
    "past_long = (\n",
    "    g[\"_watch_on_click\"].expanding()\n",
    "    .apply(lambda s: (s >= 30000).sum(), raw=True)\n",
    "    .shift(1)\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "past_clicks = (\n",
    "    g[\"_watch_on_click\"].expanding()\n",
    "    .count()\n",
    "    .shift(1)\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "df_final_sample[\"pct_long_watch_streamer_30s\"] = (\n",
    "    past_long / np.where(past_clicks == 0, np.nan, past_clicks)\n",
    ")\n",
    "\n",
    "# missing flags\n",
    "df_final_sample[\"watch_time_streamer_missing\"] = (\n",
    "    df_final_sample[[\"avg_watch_time_streamer\", \"median_watch_time_streamer\"]].isna().any(axis=1).astype(\"int64\")\n",
    ")\n",
    "\n",
    "# fill NaNs with 0 (paired with missing flags)\n",
    "fill_cols = [\n",
    "    \"avg_watch_time_streamer\",\n",
    "    \"median_watch_time_streamer\",\n",
    "    \"pct_long_watch_streamer_30s\",\n",
    "]\n",
    "df_final_sample[fill_cols] = df_final_sample[fill_cols].fillna(0)\n",
    "\n",
    "# cleanup helper\n",
    "df_final_sample = df_final_sample.drop(columns=[\"_watch_on_click\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b377f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 6.3.3 streamer engagement quality features\n",
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"live_id\",\n",
    "    \"streamer_id\",\n",
    "    \"avg_watch_time_streamer\",\n",
    "    \"median_watch_time_streamer\",\n",
    "    \"pct_long_watch_streamer_30s\",\n",
    "    \"watch_time_streamer_missing\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "feat_cols = [\n",
    "    \"avg_watch_time_streamer\",\n",
    "    \"median_watch_time_streamer\",\n",
    "    \"pct_long_watch_streamer_30s\",\n",
    "    \"watch_time_streamer_missing\",\n",
    "]\n",
    "\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100),\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "display(df_final_sample[feat_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc8946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3.4 Streamer interaction volume (past-only, 7d)\n",
    "df_final_sample = df_final_sample.sort_values([\"streamer_id\", \"imp_timestamp\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "g = df_final_sample.groupby(\"streamer_id\", group_keys=False)\n",
    "\n",
    "def _roll_sum(col, w):\n",
    "    return (\n",
    "        g.rolling(w, on=\"imp_timestamp\", closed=\"left\")[col]\n",
    "        .sum()\n",
    "        .reset_index(level=0, drop=True)\n",
    "        .to_numpy()\n",
    "    )\n",
    "\n",
    "df_final_sample[\"num_comment_streamer_7d\"] = _roll_sum(\"is_comment\", \"7d\")\n",
    "df_final_sample[\"num_like_streamer_7d\"] = _roll_sum(\"is_like\", \"7d\")\n",
    "df_final_sample[\"amount_gift_streamer_7d\"] = _roll_sum(\"gift_price\", \"7d\")\n",
    "\n",
    "fill_cols = [\n",
    "    \"num_comment_streamer_7d\",\n",
    "    \"num_like_streamer_7d\",\n",
    "    \"amount_gift_streamer_7d\",\n",
    "]\n",
    "df_final_sample[fill_cols] = df_final_sample[fill_cols].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2612a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 6.3.4 streamer interaction volume features\n",
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"live_id\",\n",
    "    \"streamer_id\",\n",
    "    \"num_comment_streamer_7d\",\n",
    "    \"num_like_streamer_7d\",\n",
    "    \"amount_gift_streamer_7d\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "feat_cols = [\n",
    "    \"num_comment_streamer_7d\",\n",
    "    \"num_like_streamer_7d\",\n",
    "    \"amount_gift_streamer_7d\",\n",
    "]\n",
    "\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100),\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "display(df_final_sample[feat_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade1a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 feature list (6.3.16.3.4)\n",
    "streamer_feat_6_3 = [\n",
    "    # 6.3.1\n",
    "    \"streamer_account_age\",\n",
    "    \"streamer_live_age\",\n",
    "    # 6.3.2\n",
    "    \"ctr_streamer_1d\",\n",
    "    \"ctr_streamer_7d\",\n",
    "    \"num_imp_streamer_7d\",\n",
    "    \"num_click_streamer_7d\",\n",
    "    \"num_lives_streamer_7d\",\n",
    "    # 6.3.3\n",
    "    \"avg_watch_time_streamer\",\n",
    "    \"median_watch_time_streamer\",\n",
    "    \"pct_long_watch_streamer_30s\",\n",
    "    \"watch_time_streamer_missing\",\n",
    "    # 6.3.4\n",
    "    \"num_comment_streamer_7d\",\n",
    "    \"num_like_streamer_7d\",\n",
    "    \"amount_gift_streamer_7d\",\n",
    "]\n",
    "\n",
    "print(\"Total engineered streamer features (6.3.16.3.4):\", len(streamer_feat_6_3))\n",
    "\n",
    "# handle dropped/missing columns\n",
    "missing_cols = [c for c in streamer_feat_6_3 if c not in df_final_sample.columns]\n",
    "present_cols = [c for c in streamer_feat_6_3 if c in df_final_sample.columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_cols)\n",
    "\n",
    "# preview\n",
    "cols = [\"user_id\", \"live_id\", \"streamer_id\", \"imp_timestamp\"] + present_cols\n",
    "with pd.option_context(\"display.max_columns\", None, \"display.width\", None):\n",
    "    display(df_final_sample[cols].head())\n",
    "\n",
    "# missingness + summary\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[present_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[present_cols].isna().mean().mul(100),\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "display(df_final_sample[present_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b5c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"user_id\", \"streamer_id\", \"live_id\", \"imp_timestamp\"] + streamer_feat_6_3\n",
    "\n",
    "with pd.option_context(\"display.max_columns\", None, \"display.width\", None):\n",
    "    display(df_final_sample[cols].head())\n",
    "\n",
    "df_final_sample[cols].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2089903",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e309f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 6.4 Cross Features (High ROI)\n",
    "##### 6.4.1 User x streamer\n",
    "\t\t\t# ctr_user_streamer_7d\n",
    "\t\t\t# num_click_user_streamer_7d\n",
    "\t\t\t# num_imp_user_streamer_7d\n",
    "\t\t\t# time_since_last_impression_user_streamer\n",
    "\t\t\t# time_since_last_click_user_streamer\n",
    "##### 6.4.2 User x category\n",
    "\t\t\t# ctr_user_category_7d\n",
    "\t\t\t# num_click_user_category_7d\n",
    "\t\t\t# num_imp_user_category_7d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216d65aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4.1 User x Streamer features (past-only, leakage-safe)\n",
    "df_final_sample = df_final_sample.sort_values(\n",
    "    [\"user_id\", \"streamer_id\", \"imp_timestamp\"], kind=\"mergesort\"\n",
    ").reset_index(drop=True)\n",
    "\n",
    "g = df_final_sample.groupby([\"user_id\", \"streamer_id\"], group_keys=False)\n",
    "\n",
    "# helper: rolling counts with time windows (past only)\n",
    "def _roll_cnt(w):\n",
    "    return (\n",
    "        g.rolling(w, on=\"imp_timestamp\", closed=\"left\")[\"is_click\"]\n",
    "        .count()\n",
    "        .reset_index(level=[0, 1], drop=True)\n",
    "        .to_numpy()\n",
    "    )\n",
    "\n",
    "def _roll_sum(w):\n",
    "    return (\n",
    "        g.rolling(w, on=\"imp_timestamp\", closed=\"left\")[\"is_click\"]\n",
    "        .sum()\n",
    "        .reset_index(level=[0, 1], drop=True)\n",
    "        .to_numpy()\n",
    "    )\n",
    "\n",
    "# 7d window counts\n",
    "imp_7d = _roll_cnt(\"7d\")\n",
    "click_7d = _roll_sum(\"7d\")\n",
    "\n",
    "df_final_sample[\"num_imp_user_streamer_7d\"] = imp_7d\n",
    "df_final_sample[\"num_click_user_streamer_7d\"] = click_7d\n",
    "df_final_sample[\"ctr_user_streamer_7d\"] = (click_7d / np.where(imp_7d == 0, np.nan, imp_7d))\n",
    "\n",
    "# recency: time since last impression/click (in seconds)\n",
    "last_imp = g[\"imp_timestamp\"].shift(1)\n",
    "df_final_sample[\"time_since_last_impression_user_streamer\"] = (\n",
    "    (df_final_sample[\"imp_timestamp\"] - last_imp).dt.total_seconds()\n",
    ")\n",
    "\n",
    "last_click_ts = df_final_sample[\"imp_timestamp\"].where(df_final_sample[\"is_click\"] == 1)\n",
    "last_click_ts = g[\"imp_timestamp\"].apply(lambda s: s.where(df_final_sample.loc[s.index, \"is_click\"] == 1)).ffill().shift(1)\n",
    "df_final_sample[\"time_since_last_click_user_streamer\"] = (\n",
    "    (df_final_sample[\"imp_timestamp\"] - last_click_ts).dt.total_seconds()\n",
    ")\n",
    "\n",
    "# missing flags (first impression/click)\n",
    "df_final_sample[\"tsli_user_streamer_missing\"] = df_final_sample[\"time_since_last_impression_user_streamer\"].isna().astype(\"int64\")\n",
    "df_final_sample[\"tslc_user_streamer_missing\"] = df_final_sample[\"time_since_last_click_user_streamer\"].isna().astype(\"int64\")\n",
    "\n",
    "# fill NaNs with 0\n",
    "fill_cols = [\n",
    "    \"num_imp_user_streamer_7d\",\n",
    "    \"num_click_user_streamer_7d\",\n",
    "    \"ctr_user_streamer_7d\",\n",
    "    \"time_since_last_impression_user_streamer\",\n",
    "    \"time_since_last_click_user_streamer\",\n",
    "]\n",
    "df_final_sample[fill_cols] = df_final_sample[fill_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727920cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 6.4.1 user x streamer features\n",
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"streamer_id\",\n",
    "    \"live_id\",\n",
    "    \"imp_timestamp\",\n",
    "    \"num_imp_user_streamer_7d\",\n",
    "    \"num_click_user_streamer_7d\",\n",
    "    \"ctr_user_streamer_7d\",\n",
    "    \"time_since_last_impression_user_streamer\",\n",
    "    \"time_since_last_click_user_streamer\",\n",
    "    \"tsli_user_streamer_missing\",\n",
    "    \"tslc_user_streamer_missing\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "feat_cols = [\n",
    "    \"num_imp_user_streamer_7d\",\n",
    "    \"num_click_user_streamer_7d\",\n",
    "    \"ctr_user_streamer_7d\",\n",
    "    \"time_since_last_impression_user_streamer\",\n",
    "    \"time_since_last_click_user_streamer\",\n",
    "    \"tsli_user_streamer_missing\",\n",
    "    \"tslc_user_streamer_missing\",\n",
    "]\n",
    "\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100),\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "display(df_final_sample[feat_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c10850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4.2 User x Category features (past-only, 7d)\n",
    "cat_col = \"live_content_category_le\"  # change if your category column name differs\n",
    "\n",
    "df_final_sample = df_final_sample.sort_values(\n",
    "    [\"user_id\", cat_col, \"imp_timestamp\"], kind=\"mergesort\"\n",
    ").reset_index(drop=True)\n",
    "\n",
    "g = df_final_sample.groupby([\"user_id\", cat_col], group_keys=False)\n",
    "\n",
    "def _roll_cnt(w):\n",
    "    return (\n",
    "        g.rolling(w, on=\"imp_timestamp\", closed=\"left\")[\"is_click\"]\n",
    "        .count()\n",
    "        .reset_index(level=[0, 1], drop=True)\n",
    "        .to_numpy()\n",
    "    )\n",
    "\n",
    "def _roll_sum(w):\n",
    "    return (\n",
    "        g.rolling(w, on=\"imp_timestamp\", closed=\"left\")[\"is_click\"]\n",
    "        .sum()\n",
    "        .reset_index(level=[0, 1], drop=True)\n",
    "        .to_numpy()\n",
    "    )\n",
    "\n",
    "imp_7d = _roll_cnt(\"7d\")\n",
    "click_7d = _roll_sum(\"7d\")\n",
    "\n",
    "df_final_sample[\"num_imp_user_category_7d\"] = imp_7d\n",
    "df_final_sample[\"num_click_user_category_7d\"] = click_7d\n",
    "df_final_sample[\"ctr_user_category_7d\"] = (click_7d / np.where(imp_7d == 0, np.nan, imp_7d))\n",
    "\n",
    "# fill missing with 0\n",
    "fill_cols = [\n",
    "    \"num_imp_user_category_7d\",\n",
    "    \"num_click_user_category_7d\",\n",
    "    \"ctr_user_category_7d\",\n",
    "]\n",
    "df_final_sample[fill_cols] = df_final_sample[fill_cols].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1cc2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 6.4.2 user x category features\n",
    "cat_col = \"live_content_category_le\"  # change if needed\n",
    "\n",
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"streamer_id\",\n",
    "    \"live_id\",\n",
    "    \"imp_timestamp\",\n",
    "    cat_col,\n",
    "    \"num_imp_user_category_7d\",\n",
    "    \"num_click_user_category_7d\",\n",
    "    \"ctr_user_category_7d\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "feat_cols = [\n",
    "    \"num_imp_user_category_7d\",\n",
    "    \"num_click_user_category_7d\",\n",
    "    \"ctr_user_category_7d\",\n",
    "]\n",
    "\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100),\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "display(df_final_sample[feat_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c7374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 6.4.2 user x category features\n",
    "cat_col = \"live_content_category_le\"  # change if needed\n",
    "\n",
    "cols = [\n",
    "    \"user_id\",\n",
    "    \"streamer_id\",\n",
    "    \"live_id\",\n",
    "    \"imp_timestamp\",\n",
    "    cat_col,\n",
    "    \"num_imp_user_category_7d\",\n",
    "    \"num_click_user_category_7d\",\n",
    "    \"ctr_user_category_7d\",\n",
    "]\n",
    "\n",
    "display(df_final_sample[cols].head())\n",
    "\n",
    "feat_cols = [\n",
    "    \"num_imp_user_category_7d\",\n",
    "    \"num_click_user_category_7d\",\n",
    "    \"ctr_user_category_7d\",\n",
    "]\n",
    "\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[feat_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[feat_cols].isna().mean().mul(100),\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "display(df_final_sample[feat_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4.16.4.2 feature list\n",
    "user_x_feat_6_4 = [\n",
    "    # 6.4.1 user x streamer\n",
    "    \"num_imp_user_streamer_7d\",\n",
    "    \"num_click_user_streamer_7d\",\n",
    "    \"ctr_user_streamer_7d\",\n",
    "    \"time_since_last_impression_user_streamer\",\n",
    "    \"time_since_last_click_user_streamer\",\n",
    "    \"tsli_user_streamer_missing\",\n",
    "    \"tslc_user_streamer_missing\",\n",
    "    # 6.4.2 user x category\n",
    "    \"num_imp_user_category_7d\",\n",
    "    \"num_click_user_category_7d\",\n",
    "    \"ctr_user_category_7d\",\n",
    "]\n",
    "\n",
    "print(\"Total engineered user-x features (6.4.16.4.2):\", len(user_x_feat_6_4))\n",
    "\n",
    "missing_cols = [c for c in user_x_feat_6_4 if c not in df_final_sample.columns]\n",
    "present_cols = [c for c in user_x_feat_6_4 if c in df_final_sample.columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_cols)\n",
    "\n",
    "cols = [\"user_id\", \"streamer_id\", \"live_id\", \"imp_timestamp\"] + present_cols\n",
    "with pd.option_context(\"display.max_columns\", None, \"display.width\", None):\n",
    "    display(df_final_sample[cols].head())\n",
    "\n",
    "missing = pd.DataFrame({\n",
    "    \"missing_count\": df_final_sample[present_cols].isna().sum(),\n",
    "    \"missing_pct\": df_final_sample[present_cols].isna().mean().mul(100),\n",
    "})\n",
    "display(missing)\n",
    "\n",
    "display(df_final_sample[present_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3420b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"user_id\", \"streamer_id\", \"live_id\", \"imp_timestamp\"] + user_x_feat_6_4\n",
    "\n",
    "with pd.option_context(\"display.max_columns\", None, \"display.width\", None):\n",
    "    display(df_final_sample[cols].head())\n",
    "\n",
    "df_final_sample[cols].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88dd591",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de5bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count features created in 6.16.4\n",
    "\n",
    "feat_6_1 = [\n",
    "    # 6.1 User features (as defined in your notebook)\n",
    "    \"user_account_age\",\n",
    "    \"user_watch_live_age\",\n",
    "    \"ctr_user_15min\", \"ctr_user_3hr\", \"ctr_user_1d\", \"ctr_user_7d\",\n",
    "    \"num_imp_user_10min\", \"num_imp_user_30min\", \"num_imp_user_2hr\", \"num_imp_user_12hr\", \"num_imp_user_1d\", \"num_imp_user_7d\",\n",
    "    \"num_click_user_15min\", \"num_click_user_3hr\", \"num_click_user_1d\", \"num_click_user_7d\",\n",
    "    \"click_trend_user\",\n",
    "    \"time_since_last_impression_user\", \"tsli_missing\",\n",
    "    \"time_since_last_click_user\", \"tslc_missing\",\n",
    "    \"consecutive_skips_user\",\n",
    "    \"avg_watch_time_user\", \"median_watch_time_user\", \"pct_long_watch_user_30s\",\n",
    "    \"comment_rate_user\", \"has_comment_user_24h\", \"num_comment_user_24h\",\n",
    "    \"like_rate_user\", \"has_like_user_24h\", \"num_like_user_24h\",\n",
    "    \"has_gift_user_7d\", \"num_gift_user_7d\", \"amount_gift_user_7d\",\n",
    "]\n",
    "\n",
    "feat_6_2 = room_feat_6_2  # from your earlier block\n",
    "\n",
    "feat_6_3 = [\n",
    "    # 6.3.1\n",
    "    \"streamer_account_age\",\n",
    "    \"streamer_live_age\",\n",
    "    \"streamer_account_age_missing\",\n",
    "    \"streamer_live_age_missing\",\n",
    "    # 6.3.2\n",
    "    \"ctr_streamer_1d\",\n",
    "    \"ctr_streamer_7d\",\n",
    "    \"num_imp_streamer_7d\",\n",
    "    \"num_click_streamer_7d\",\n",
    "    \"num_lives_streamer_7d\",\n",
    "    # 6.3.3\n",
    "    \"avg_watch_time_streamer\",\n",
    "    \"median_watch_time_streamer\",\n",
    "    \"pct_long_watch_streamer_30s\",\n",
    "    \"watch_time_streamer_missing\",\n",
    "    # 6.3.4\n",
    "    \"num_comment_streamer_7d\",\n",
    "    \"num_like_streamer_7d\",\n",
    "    \"amount_gift_streamer_7d\",\n",
    "]\n",
    "\n",
    "feat_6_4 = [\n",
    "    # 6.4.1 user x streamer\n",
    "    \"num_imp_user_streamer_7d\",\n",
    "    \"num_click_user_streamer_7d\",\n",
    "    \"ctr_user_streamer_7d\",\n",
    "    \"time_since_last_impression_user_streamer\",\n",
    "    \"time_since_last_click_user_streamer\",\n",
    "    \"tsli_user_streamer_missing\",\n",
    "    \"tslc_user_streamer_missing\",\n",
    "    # 6.4.2 user x category\n",
    "    \"num_imp_user_category_7d\",\n",
    "    \"num_click_user_category_7d\",\n",
    "    \"ctr_user_category_7d\",\n",
    "    # 6.4.3 user x room (dropped if removed)\n",
    "    \"num_imp_user_room_1d\",\n",
    "    \"num_click_user_room_7d\",\n",
    "    \"time_since_last_impression_user_room\",\n",
    "    \"tsli_user_room_missing\",\n",
    "]\n",
    "\n",
    "all_feats = feat_6_1 + feat_6_2 + feat_6_3 + feat_6_4\n",
    "\n",
    "print(\"Total features (6.16.4):\", len(all_feats))\n",
    "present = [c for c in all_feats if c in df_final_sample.columns]\n",
    "missing = [c for c in all_feats if c not in df_final_sample.columns]\n",
    "\n",
    "print(\"Present:\", len(present))\n",
    "print(\"Missing:\", len(missing))\n",
    "print(\"Missing columns:\", missing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc5389",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_sample_transformed = df_final_sample.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf382c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def explore_numeric_feature(df, col, bins=50, iqr_k=1.5, clip_q=None):\n",
    "    \"\"\"\n",
    "    Explore one numeric feature:\n",
    "    1) distribution + skewness (summary + plots)\n",
    "    2) outliers (IQR rule)\n",
    "    \n",
    "    Params:\n",
    "      df: DataFrame\n",
    "      col: column name\n",
    "      bins: histogram bins\n",
    "      iqr_k: multiplier for IQR outlier bounds\n",
    "      clip_q: optional quantile clip for plotting (e.g., 0.995)\n",
    "    \"\"\"\n",
    "    if col not in df.columns:\n",
    "        print(f\"[ERROR] Column not found: {col}\")\n",
    "        return\n",
    "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "        print(f\"[ERROR] Column is not numeric: {col}\")\n",
    "        return\n",
    "\n",
    "    s = df[col].dropna()\n",
    "    if s.empty:\n",
    "        print(f\"[WARN] Column is all NaN: {col}\")\n",
    "        return\n",
    "\n",
    "    # basic stats\n",
    "    skew = s.skew()\n",
    "    summary = s.describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99])\n",
    "\n",
    "    print(f\"Feature: {col}\")\n",
    "    print(f\"Count (non-null): {len(s)}\")\n",
    "    print(f\"Skewness: {skew:.6f}\")\n",
    "    display(summary)\n",
    "\n",
    "    # optional clipping for plotting only\n",
    "    plot_s = s\n",
    "    if clip_q is not None:\n",
    "        hi = s.quantile(clip_q)\n",
    "        lo = s.quantile(1 - clip_q) if clip_q > 0.5 else s.quantile(clip_q)\n",
    "        plot_s = s.clip(lower=lo, upper=hi)\n",
    "\n",
    "    # plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    axes[0].hist(plot_s, bins=bins)\n",
    "    axes[0].set_title(f\"{col} Histogram\")\n",
    "    axes[1].boxplot(plot_s, vert=False, showfliers=True)\n",
    "    axes[1].set_title(f\"{col} Boxplot\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # outlier check (IQR)\n",
    "    q1, q3 = np.percentile(s, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    if iqr == 0:\n",
    "        print(\"Outlier check: IQR = 0 (no outliers by IQR rule).\")\n",
    "        return\n",
    "\n",
    "    lo = q1 - iqr_k * iqr\n",
    "    hi = q3 + iqr_k * iqr\n",
    "    outliers = s[(s < lo) | (s > hi)]\n",
    "\n",
    "    print(f\"Outlier bounds: [{lo:.6f}, {hi:.6f}]\")\n",
    "    print(f\"Outlier count: {len(outliers)} ({len(outliers) / len(s) * 100:.4f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae483f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_numeric_feature(\n",
    "    df,\n",
    "    col,\n",
    "    method=\"zscore\",   # \"zscore\", \"minmax\", \"robust\"\n",
    "    clip_q=None,       # e.g., 0.995 to clip tails before scaling\n",
    "    eps=1e-9,\n",
    "):\n",
    "    \"\"\"\n",
    "    Normalize/standardize a numeric feature.\n",
    "    Returns (scaled_series, params_dict).\n",
    "\n",
    "    method:\n",
    "      - \"zscore\": (x - mean) / std\n",
    "      - \"minmax\": (x - min) / (max - min)\n",
    "      - \"robust\": (x - median) / IQR\n",
    "    \"\"\"\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column not found: {col}\")\n",
    "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "        raise TypeError(f\"Column is not numeric: {col}\")\n",
    "\n",
    "    s = df[col].astype(float)\n",
    "\n",
    "    # optional clipping for stability\n",
    "    if clip_q is not None:\n",
    "        lo = s.quantile(1 - clip_q)\n",
    "        hi = s.quantile(clip_q)\n",
    "        s = s.clip(lower=lo, upper=hi)\n",
    "\n",
    "    if method == \"zscore\":\n",
    "        mean = s.mean()\n",
    "        std = s.std()\n",
    "        std = std if std > eps else eps\n",
    "        scaled = (s - mean) / std\n",
    "        params = {\"method\": method, \"mean\": mean, \"std\": std}\n",
    "\n",
    "    elif method == \"minmax\":\n",
    "        minv = s.min()\n",
    "        maxv = s.max()\n",
    "        denom = (maxv - minv) if (maxv - minv) > eps else eps\n",
    "        scaled = (s - minv) / denom\n",
    "        params = {\"method\": method, \"min\": minv, \"max\": maxv}\n",
    "\n",
    "    elif method == \"robust\":\n",
    "        med = s.median()\n",
    "        q1 = s.quantile(0.25)\n",
    "        q3 = s.quantile(0.75)\n",
    "        iqr = (q3 - q1) if (q3 - q1) > eps else eps\n",
    "        scaled = (s - med) / iqr\n",
    "        params = {\"method\": method, \"median\": med, \"q1\": q1, \"q3\": q3, \"iqr\": iqr}\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"method must be one of: 'zscore', 'minmax', 'robust'\")\n",
    "\n",
    "    return scaled, params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837cd61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def log_transform_feature(df, col, method=\"log1p\", add_shift_if_negative=True):\n",
    "    \"\"\"\n",
    "    Log-transform a numeric feature.\n",
    "    - method: \"log1p\" or \"log\"\n",
    "    - add_shift_if_negative: if True, shifts data so min >= 0 before log1p/log\n",
    "    Returns (transformed_series, params_dict)\n",
    "    \"\"\"\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column not found: {col}\")\n",
    "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "        raise TypeError(f\"Column is not numeric: {col}\")\n",
    "\n",
    "    s = df[col].astype(float)\n",
    "    shift = 0.0\n",
    "\n",
    "    if add_shift_if_negative:\n",
    "        min_val = s.min()\n",
    "        if min_val < 0:\n",
    "            shift = -min_val\n",
    "            s = s + shift\n",
    "\n",
    "    if method == \"log1p\":\n",
    "        transformed = np.log1p(s)\n",
    "    elif method == \"log\":\n",
    "        # avoid log(0)\n",
    "        transformed = np.log(s.replace(0, np.nan))\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'log1p' or 'log'\")\n",
    "\n",
    "    params = {\"method\": method, \"shift\": shift}\n",
    "    return transformed, params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f896dad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_transform(x, eps=1e-6):\n",
    "    x = np.clip(x, eps, 1 - eps)\n",
    "    return np.log(x / (1 - x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8809fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration for 6. 1 User features\n",
    "# 6.1.1\n",
    "# explore_numeric_feature(df_final_sample_transformed, \"user_account_age\")\n",
    "# explore_numeric_feature(df_final_sample_transformed, \"user_watch_live_age\")\n",
    "\n",
    "# 6.1.2\n",
    "# for col in [\"ctr_user_15min\", \"ctr_user_3hr\", \"ctr_user_1d\", \"ctr_user_7d\"]:\n",
    "#     explore_numeric_feature(df_final_sample, col)\n",
    "\n",
    "# 6.1.3\n",
    "# for col in [\n",
    "#     \"num_imp_user_10min\",\n",
    "#     \"num_imp_user_30min\",\n",
    "#     \"num_imp_user_2hr\",\n",
    "#     \"num_imp_user_12hr\",\n",
    "#     \"num_imp_user_1d\",\n",
    "#     \"num_imp_user_7d\",\n",
    "# ]:\n",
    "#     explore_numeric_feature(df_final_sample_transformed, col)\n",
    "\n",
    "# 6.1.4\n",
    "# for col in [\n",
    "#     \"num_click_user_15min\",\n",
    "#     \"num_click_user_3hr\",\n",
    "#     \"num_click_user_1d\",\n",
    "#     \"num_click_user_7d\",\n",
    "#     \"click_trend_user\",\n",
    "# ]:\n",
    "#     explore_numeric_feature(df_final_sample_transformed, col)\n",
    "\n",
    "# 6.1.5\n",
    "# for col in [\n",
    "#     \"time_since_last_impression_user\",\n",
    "#     \"time_since_last_click_user\",\n",
    "#     \"consecutive_skips_user\",\n",
    "# ]:\n",
    "#     explore_numeric_feature(df_final_sample_transformed, col)\n",
    "\n",
    "# 6.1.6\n",
    "# for col in [\"avg_watch_time_user\", \"median_watch_time_user\", \"pct_long_watch_user_30s\"]:\n",
    "#     explore_numeric_feature(df_final_sample_transformed, col)\n",
    "\n",
    "# 6.1.7\n",
    "# # numeric\n",
    "# for col in [\"comment_rate_user\", \"num_comment_user_24h\"]:\n",
    "#     explore_numeric_feature(df_final_sample_transformed, col)\n",
    "\n",
    "\n",
    "# 6.1.8 \n",
    "# # numeric\n",
    "# for col in [\"like_rate_user\", \"num_like_user_24h\"]:\n",
    "#     explore_numeric_feature(df_final_sample_transformed, col)\n",
    "\n",
    "# 6.1.9\n",
    "# # numeric\n",
    "# for col in [\"num_gift_user_7d\", \"amount_gift_user_7d\"]:\n",
    "#     explore_numeric_feature(df_final_sample_transformed, col)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 6.2.1\n",
    "# explore_numeric_feature(df_final_sample_transformed, \"time_since_live_start\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0994d3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 \n",
    "# .............................................standardize \n",
    "# user_account_age\n",
    "# user_whatch_live_age\n",
    "\n",
    "# .............................................clip_0, 1\n",
    "# ctr_user_15min\n",
    "# ctr_user_3hr\n",
    "# ctr_user_1d\n",
    "# ctr_user_7d\n",
    "# comment_rate_user\n",
    "# like_rate_user\n",
    "\n",
    "#.............................................log1p + clip p99 + standardize\n",
    "# num_imp_user_10min\n",
    "# num_imp_user_30min\n",
    "# num_imp_user_2hr\n",
    "# num_imp_user_12hr\n",
    "# num_imp_user_1d\n",
    "# num_imp_user_7d\n",
    "\n",
    "# num_click_user_15min\n",
    "# num_click_user_3hr\n",
    "# num_click_user_1d\n",
    "# num_click_user_7d\n",
    "\n",
    "# time_since_last_impression_user\n",
    "# time_since_last_click_user\n",
    "# consecutive_skips_user\n",
    "\n",
    "# avg_watch_time_user\n",
    "# median_watch_time_user\n",
    "# num_comment_user_24h\n",
    "# num_like_user_24h\n",
    "\n",
    "# num_gift_user_7d\n",
    "# amount_gift_user_7d\n",
    "\n",
    "#............................................clip p1-p99 + standardize\n",
    "# click_trend_user\n",
    "\n",
    "\n",
    "#.............................................leave as is\n",
    "# tsli_missing \n",
    "# tslc_missing\n",
    "# avg_watch_time_user_missing\n",
    "# median_watch_time_user_missing\n",
    "# pct_long_watch_user_30s\n",
    "# has_comment_user_24h\n",
    "# has_like_user_24h\n",
    "# has_gift_user_7d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 6.2 \n",
    "# .............................................log1p + standardize\n",
    "# time_since_live_start\n",
    "# time_since_start_live\n",
    "\n",
    "# .............................................clip_0, 1\n",
    "# ctr_room_10min\n",
    "# ctr_room_30min\n",
    "# ctr_room_2hr\n",
    "# ctr_room_12hr\n",
    "\n",
    "# comment_rate_live\n",
    "# comment_rate_live_15min\n",
    "# comment_rate_live_1hr\n",
    "# comment_rate_live_3hr\n",
    "\n",
    "# like_rate_live\n",
    "# like_rate_live_15min\n",
    "# like_rate_live_1hr\n",
    "# like_rate_live_3hr\n",
    "\n",
    "# gift_rate_live\n",
    "# gift_rate_live_15min\n",
    "# gift_rate_live_1hr\n",
    "# gift_rate_live_3hr\n",
    "\n",
    "\n",
    "# .............................................log1p + clip p99 + standardize\n",
    "# num_imp_room_10min\n",
    "# num_imp_room_30min\n",
    "# num_imp_room_2hr\n",
    "# num_imp_room_12hr\n",
    "# num_imp_room_1d\n",
    "\n",
    "# num_click_room_10min\n",
    "# num_click_room_30min\n",
    "# num_click_room_2hr\n",
    "# num_click_room_12hr\n",
    "# num_click_room_1d\n",
    "\n",
    "# num_comment_live\n",
    "# num_comment_live_15min\n",
    "# num_comment_live_1hr\n",
    "# num_comment_live_3hr\n",
    "\n",
    "# num_like_live\n",
    "# num_like_live_15min\n",
    "# num_like_live_1hr\n",
    "# num_like_live_3hr\n",
    "\n",
    "# num_gift_live\n",
    "# num_gift_live_15min\n",
    "# num_gift_live_1hr\n",
    "# num_gift_live_3hr\n",
    "\n",
    "# amount_gift_live\n",
    "# amount_gift_live_15min\n",
    "# amount_gift_live_1hr\n",
    "# amount_gift_live_3hr\n",
    "\n",
    "\n",
    "# ............................................clip p1-p99 + standardize\n",
    "# ctr_trend_room\n",
    "# comment_trend_room\n",
    "# like_trend_room\n",
    "# gift_trend_room\n",
    "\n",
    "\n",
    "# .............................................standardize\n",
    "# avg_watch_time_live\n",
    "# median_watch_time_live\n",
    "# avg_watch_time_live_30min\n",
    "# median_watch_time_live_30min\n",
    "\n",
    "# .............................................leave as is\n",
    "# time_since_start_live_bucket\n",
    "# watch_time_live_missing\n",
    "# watch_time_live_30min_missing\n",
    "# pct_long_watch_live_60s_30min\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 6.3\n",
    "# .............................................log1p + standardize\n",
    "# streamer_account_age\n",
    "# streamer_live_age\n",
    "\n",
    "# .............................................clip_0, 1\n",
    "# ctr_streamer_1d\n",
    "# ctr_streamer_7d\n",
    "\n",
    "# .............................................log1p + clip p99 + standardize\n",
    "# num_imp_streamer_7d\n",
    "# num_click_streamer_7d\n",
    "# num_lives_streamer_7d\n",
    "\n",
    "# num_comment_streamer_7d\n",
    "# num_like_streamer_7d\n",
    "# amount_gift_streamer_7d\n",
    "\n",
    "# .............................................standardize\n",
    "# avg_watch_time_streamer\n",
    "# median_watch_time_streamer\n",
    "\n",
    "# .............................................leave as is\n",
    "# pct_long_watch_streamer_30s\n",
    "# watch_time_streamer_missing\n",
    "\n",
    "\n",
    "\n",
    "# 6.4\n",
    "# .............................................clip_0, 1\n",
    "# ctr_user_streamer_7d\n",
    "# ctr_user_category_7d\n",
    "\n",
    "# .............................................log1p + clip p99 + standardize\n",
    "# num_click_user_streamer_7d\n",
    "# num_imp_user_streamer_7d\n",
    "\n",
    "# num_click_user_category_7d\n",
    "# num_imp_user_category_7d\n",
    "\n",
    "# time_since_last_impression_user_streamer\n",
    "# time_since_last_click_user_streamer\n",
    "\n",
    "# .............................................leave as is\n",
    "# tsli_user_streamer_missing\n",
    "# tslc_user_streamer_missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946adc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation for 6. 1 User features\n",
    "# 6.1.1\n",
    "for col in [\"user_account_age\", \"user_watch_live_age\"]:\n",
    "    scaled, params = normalize_numeric_feature(df_final_sample_transformed, col, method=\"zscore\")\n",
    "    df_final_sample_transformed[col] = scaled\n",
    "\n",
    "# 6.1.2\n",
    "for col in [\"ctr_user_15min\", \"ctr_user_3hr\", \"ctr_user_1d\", \"ctr_user_7d\"]:\n",
    "    df_final_sample_transformed[col] = df_final_sample_transformed[col].clip(lower=0, upper=1)\n",
    "\n",
    "# 6.1.3\n",
    "cols = [\n",
    "    \"num_imp_user_10min\",\n",
    "    \"num_imp_user_30min\",\n",
    "    \"num_imp_user_2hr\",\n",
    "    \"num_imp_user_12hr\",\n",
    "    \"num_imp_user_1d\",\n",
    "    \"num_imp_user_7d\",\n",
    "]\n",
    "\n",
    "# log1p + clip p99\n",
    "for c in cols:\n",
    "    s = df_final_sample_transformed[c].clip(lower=0)\n",
    "    s = np.log1p(s)\n",
    "    hi = s.quantile(0.99)\n",
    "    df_final_sample_transformed[c] = s.clip(upper=hi)\n",
    "\n",
    "# standardize\n",
    "means = df_final_sample_transformed[cols].mean()\n",
    "stds = df_final_sample_transformed[cols].std().replace(0, 1.0)\n",
    "df_final_sample_transformed[cols] = (df_final_sample_transformed[cols] - means) / stds\n",
    "\n",
    "\n",
    "# 6.1.4\n",
    "cols = [\n",
    "    \"num_click_user_15min\",\n",
    "    \"num_click_user_3hr\",\n",
    "    \"num_click_user_1d\",\n",
    "    \"num_click_user_7d\",\n",
    "]\n",
    "\n",
    "# log1p + clip p99\n",
    "for c in cols:\n",
    "    s = df_final_sample_transformed[c].clip(lower=0)\n",
    "    s = np.log1p(s)\n",
    "    hi = s.quantile(0.99)\n",
    "    df_final_sample_transformed[c] = s.clip(upper=hi)\n",
    "\n",
    "# standardize\n",
    "means = df_final_sample_transformed[cols].mean()\n",
    "stds = df_final_sample_transformed[cols].std().replace(0, 1.0)\n",
    "df_final_sample_transformed[cols] = (df_final_sample_transformed[cols] - means) / stds\n",
    "\n",
    "col = \"click_trend_user\"\n",
    "s = df_final_sample_transformed[col]\n",
    "lo, hi = s.quantile(0.01), s.quantile(0.99)\n",
    "s = s.clip(lower=lo, upper=hi)\n",
    "mean = s.mean()\n",
    "std = s.std() or 1.0\n",
    "df_final_sample_transformed[col] = (s - mean) / std\n",
    "\n",
    "\n",
    "\n",
    "# 6.1.5\n",
    "cols = [\n",
    "    \"time_since_last_impression_user\",\n",
    "    \"time_since_last_click_user\",\n",
    "    \"consecutive_skips_user\",\n",
    "]\n",
    "# log1p + clip p99\n",
    "for c in cols:\n",
    "    s = df_final_sample_transformed[c].clip(lower=0)\n",
    "    s = np.log1p(s)\n",
    "    hi = s.quantile(0.99)\n",
    "    df_final_sample_transformed[c] = s.clip(upper=hi)\n",
    "\n",
    "# standardize\n",
    "means = df_final_sample_transformed[cols].mean()\n",
    "stds = df_final_sample_transformed[cols].std().replace(0, 1.0)\n",
    "df_final_sample_transformed[cols] = (df_final_sample_transformed[cols] - means) / stds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 6.1.6\n",
    "cols = [\"avg_watch_time_user\", \"median_watch_time_user\"]\n",
    "\n",
    "# log1p + clip p99\n",
    "for c in cols:\n",
    "    s = df_final_sample_transformed[c].clip(lower=0)\n",
    "    s = np.log1p(s)\n",
    "    hi = s.quantile(0.99)\n",
    "    df_final_sample_transformed[c] = s.clip(upper=hi)\n",
    "\n",
    "# standardize\n",
    "means = df_final_sample_transformed[cols].mean()\n",
    "stds = df_final_sample_transformed[cols].std().replace(0, 1.0)\n",
    "df_final_sample_transformed[cols] = (df_final_sample_transformed[cols] - means) / stds\n",
    "\n",
    "\n",
    "# 6.1.7\n",
    "# 1) clip_0,1 for comment_rate_user\n",
    "df_final_sample_transformed[\"comment_rate_user\"] = (\n",
    "    df_final_sample_transformed[\"comment_rate_user\"].clip(0, 1)\n",
    ")\n",
    "\n",
    "# 2) log1p + clip p99 + standardize for num_comment_user_24h\n",
    "col = \"num_comment_user_24h\"\n",
    "s = df_final_sample_transformed[col].clip(lower=0)\n",
    "s = np.log1p(s)\n",
    "hi = s.quantile(0.99)\n",
    "s = s.clip(upper=hi)\n",
    "\n",
    "mean = s.mean()\n",
    "std = s.std() or 1.0\n",
    "df_final_sample_transformed[col] = (s - mean) / std\n",
    "\n",
    "\n",
    "# 6.1.8\n",
    "# clip_0,1 for like_rate_user\n",
    "df_final_sample_transformed[\"like_rate_user\"] = (\n",
    "    df_final_sample_transformed[\"like_rate_user\"].clip(0, 1)\n",
    ")\n",
    "\n",
    "# log1p + clip p99 + standardize for num_like_user_24h\n",
    "col = \"num_like_user_24h\"\n",
    "s = df_final_sample_transformed[col].clip(lower=0)\n",
    "s = np.log1p(s)\n",
    "hi = s.quantile(0.99)\n",
    "s = s.clip(upper=hi)\n",
    "\n",
    "mean = s.mean()\n",
    "std = s.std() or 1.0\n",
    "df_final_sample_transformed[col] = (s - mean) / std\n",
    "\n",
    "\n",
    "# 6.1.9\n",
    "ols = [\"num_gift_user_7d\", \"amount_gift_user_7d\"]\n",
    "\n",
    "# log1p + clip p99\n",
    "for c in cols:\n",
    "    s = df_final_sample_transformed[c].clip(lower=0)\n",
    "    s = np.log1p(s)\n",
    "    hi = s.quantile(0.99)\n",
    "    df_final_sample_transformed[c] = s.clip(upper=hi)\n",
    "\n",
    "# standardize\n",
    "means = df_final_sample_transformed[cols].mean()\n",
    "stds = df_final_sample_transformed[cols].std().replace(0, 1.0)\n",
    "df_final_sample_transformed[cols] = (df_final_sample_transformed[cols] - means) / stds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9854278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation for 6.2 room features\n",
    "# -------- log1p + standardize --------\n",
    "log_std_cols = [\n",
    "    \"time_since_live_start\",\n",
    "    \"time_since_start_live\",\n",
    "]\n",
    "for c in log_std_cols:\n",
    "    s = df_final_sample_transformed[c].clip(lower=0)\n",
    "    s = np.log1p(s)\n",
    "    mean = s.mean()\n",
    "    std = s.std() or 1.0\n",
    "    df_final_sample_transformed[c] = (s - mean) / std\n",
    "\n",
    "# -------- clip_0,1 --------\n",
    "clip_01_cols = [\n",
    "    \"ctr_room_10min\", \"ctr_room_30min\", \"ctr_room_2hr\", \"ctr_room_12hr\",\n",
    "    \"comment_rate_live\", \"comment_rate_live_15min\", \"comment_rate_live_1hr\", \"comment_rate_live_3hr\",\n",
    "    \"like_rate_live\", \"like_rate_live_15min\", \"like_rate_live_1hr\", \"like_rate_live_3hr\",\n",
    "    \"gift_rate_live\", \"gift_rate_live_15min\", \"gift_rate_live_1hr\", \"gift_rate_live_3hr\",\n",
    "]\n",
    "for c in clip_01_cols:\n",
    "    df_final_sample_transformed[c] = df_final_sample_transformed[c].clip(0, 1)\n",
    "\n",
    "# -------- log1p + clip p99 + standardize --------\n",
    "log_clip_std_cols = [\n",
    "    \"num_imp_room_10min\", \"num_imp_room_30min\", \"num_imp_room_2hr\", \"num_imp_room_12hr\", \"num_imp_room_1d\",\n",
    "    \"num_click_room_10min\", \"num_click_room_30min\", \"num_click_room_2hr\", \"num_click_room_12hr\", \"num_click_room_1d\",\n",
    "    \"num_comment_live\", \"num_comment_live_15min\", \"num_comment_live_1hr\", \"num_comment_live_3hr\",\n",
    "    \"num_like_live\", \"num_like_live_15min\", \"num_like_live_1hr\", \"num_like_live_3hr\",\n",
    "    \"num_gift_live\", \"num_gift_live_15min\", \"num_gift_live_1hr\", \"num_gift_live_3hr\",\n",
    "    \"amount_gift_live\", \"amount_gift_live_15min\", \"amount_gift_live_1hr\", \"amount_gift_live_3hr\",\n",
    "]\n",
    "for c in log_clip_std_cols:\n",
    "    s = df_final_sample_transformed[c].clip(lower=0)\n",
    "    s = np.log1p(s)\n",
    "    hi = s.quantile(0.99)\n",
    "    s = s.clip(upper=hi)\n",
    "    mean = s.mean()\n",
    "    std = s.std() or 1.0\n",
    "    df_final_sample_transformed[c] = (s - mean) / std\n",
    "\n",
    "# -------- clip p1-p99 + standardize --------\n",
    "trend_cols = [\"ctr_trend_room\", \"comment_trend_room\", \"like_trend_room\", \"gift_trend_room\"]\n",
    "for c in trend_cols:\n",
    "    s = df_final_sample_transformed[c]\n",
    "    lo, hi = s.quantile(0.01), s.quantile(0.99)\n",
    "    s = s.clip(lower=lo, upper=hi)\n",
    "    mean = s.mean()\n",
    "    std = s.std() or 1.0\n",
    "    df_final_sample_transformed[c] = (s - mean) / std\n",
    "\n",
    "# -------- standardize --------\n",
    "std_cols = [\n",
    "    \"avg_watch_time_live\", \"median_watch_time_live\",\n",
    "    \"avg_watch_time_live_30min\", \"median_watch_time_live_30min\",\n",
    "]\n",
    "for c in std_cols:\n",
    "    s = df_final_sample_transformed[c]\n",
    "    mean = s.mean()\n",
    "    std = s.std() or 1.0\n",
    "    df_final_sample_transformed[c] = (s - mean) / std\n",
    "\n",
    "# leave-as-is:\n",
    "# time_since_start_live_bucket, watch_time_live_missing, watch_time_live_30min_missing, pct_long_watch_live_60s_30min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5ed43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_52 = [\n",
    "    # log1p + standardize\n",
    "    \"time_since_live_start\",\n",
    "    \"time_since_start_live\",\n",
    "\n",
    "    # clip_0,1\n",
    "    \"ctr_room_10min\", \"ctr_room_30min\", \"ctr_room_2hr\", \"ctr_room_12hr\",\n",
    "    \"comment_rate_live\", \"comment_rate_live_15min\", \"comment_rate_live_1hr\", \"comment_rate_live_3hr\",\n",
    "    \"like_rate_live\", \"like_rate_live_15min\", \"like_rate_live_1hr\", \"like_rate_live_3hr\",\n",
    "    \"gift_rate_live\", \"gift_rate_live_15min\", \"gift_rate_live_1hr\", \"gift_rate_live_3hr\",\n",
    "\n",
    "    # log1p + clip p99 + standardize\n",
    "    \"num_imp_room_10min\", \"num_imp_room_30min\", \"num_imp_room_2hr\", \"num_imp_room_12hr\", \"num_imp_room_1d\",\n",
    "    \"num_click_room_10min\", \"num_click_room_30min\", \"num_click_room_2hr\", \"num_click_room_12hr\", \"num_click_room_1d\",\n",
    "    \"num_comment_live\", \"num_comment_live_15min\", \"num_comment_live_1hr\", \"num_comment_live_3hr\",\n",
    "    \"num_like_live\", \"num_like_live_15min\", \"num_like_live_1hr\", \"num_like_live_3hr\",\n",
    "    \"num_gift_live\", \"num_gift_live_15min\", \"num_gift_live_1hr\", \"num_gift_live_3hr\",\n",
    "    \"amount_gift_live\", \"amount_gift_live_15min\", \"amount_gift_live_1hr\", \"amount_gift_live_3hr\",\n",
    "\n",
    "    # clip p1p99 + standardize\n",
    "    \"ctr_trend_room\", \"comment_trend_room\", \"like_trend_room\", \"gift_trend_room\",\n",
    "\n",
    "    # standardize\n",
    "    \"avg_watch_time_live\", \"median_watch_time_live\",\n",
    "    \"avg_watch_time_live_30min\", \"median_watch_time_live_30min\",\n",
    "]\n",
    "\n",
    "# for col in features_52:\n",
    "#     explore_numeric_feature(df_final_sample_transformed, col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084a9b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_sample_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cccbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation for 6.3 room features\n",
    "\n",
    "# -------- log1p + standardize --------\n",
    "log_std_cols = [\n",
    "    \"streamer_account_age\",\n",
    "    \"streamer_live_age\",\n",
    "]\n",
    "for c in log_std_cols:\n",
    "    s = df_final_sample_transformed[c].clip(lower=0)\n",
    "    s = np.log1p(s)\n",
    "    mean = s.mean()\n",
    "    std = s.std() or 1.0\n",
    "    df_final_sample_transformed[c] = (s - mean) / std\n",
    "\n",
    "# -------- clip_0,1 --------\n",
    "clip_01_cols = [\n",
    "    \"ctr_streamer_1d\",\n",
    "    \"ctr_streamer_7d\",\n",
    "]\n",
    "for c in clip_01_cols:\n",
    "    df_final_sample_transformed[c] = df_final_sample_transformed[c].clip(0, 1)\n",
    "\n",
    "# -------- log1p + clip p99 + standardize --------\n",
    "log_clip_std_cols = [\n",
    "    \"num_imp_streamer_7d\",\n",
    "    \"num_click_streamer_7d\",\n",
    "    \"num_lives_streamer_7d\",\n",
    "    \"num_comment_streamer_7d\",\n",
    "    \"num_like_streamer_7d\",\n",
    "    \"amount_gift_streamer_7d\",\n",
    "]\n",
    "for c in log_clip_std_cols:\n",
    "    s = df_final_sample_transformed[c].clip(lower=0)\n",
    "    s = np.log1p(s)\n",
    "    hi = s.quantile(0.99)\n",
    "    s = s.clip(upper=hi)\n",
    "    mean = s.mean()\n",
    "    std = s.std() or 1.0\n",
    "    df_final_sample_transformed[c] = (s - mean) / std\n",
    "\n",
    "# -------- standardize --------\n",
    "std_cols = [\n",
    "    \"avg_watch_time_streamer\",\n",
    "    \"median_watch_time_streamer\",\n",
    "]\n",
    "for c in std_cols:\n",
    "    s = df_final_sample_transformed[c]\n",
    "    mean = s.mean()\n",
    "    std = s.std() or 1.0\n",
    "    df_final_sample_transformed[c] = (s - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff3cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_sample_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d13345",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_12 = [\n",
    "    # log1p + standardize\n",
    "    \"streamer_account_age\",\n",
    "    \"streamer_live_age\",\n",
    "\n",
    "    # clip_0,1\n",
    "    \"ctr_streamer_1d\",\n",
    "    \"ctr_streamer_7d\",\n",
    "\n",
    "    # log1p + clip p99 + standardize\n",
    "    \"num_imp_streamer_7d\",\n",
    "    \"num_click_streamer_7d\",\n",
    "    \"num_lives_streamer_7d\",\n",
    "    \"num_comment_streamer_7d\",\n",
    "    \"num_like_streamer_7d\",\n",
    "    \"amount_gift_streamer_7d\",\n",
    "\n",
    "    # standardize\n",
    "    \"avg_watch_time_streamer\",\n",
    "    \"median_watch_time_streamer\",\n",
    "]\n",
    "\n",
    "# for col in features_12:\n",
    "#     explore_numeric_feature(df_final_sample_transformed, col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7844ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation for 6.4 room features\n",
    "\n",
    "# -------- clip_0,1 --------\n",
    "clip_01_cols = [\n",
    "    \"ctr_user_streamer_7d\",\n",
    "    \"ctr_user_category_7d\",\n",
    "]\n",
    "for c in clip_01_cols:\n",
    "    df_final_sample_transformed[c] = df_final_sample_transformed[c].clip(0, 1)\n",
    "\n",
    "# -------- log1p + clip p99 + standardize --------\n",
    "log_clip_std_cols = [\n",
    "    \"num_click_user_streamer_7d\",\n",
    "    \"num_imp_user_streamer_7d\",\n",
    "    \"num_click_user_category_7d\",\n",
    "    \"num_imp_user_category_7d\",\n",
    "    \"time_since_last_impression_user_streamer\",\n",
    "    \"time_since_last_click_user_streamer\",\n",
    "]\n",
    "for c in log_clip_std_cols:\n",
    "    s = df_final_sample_transformed[c].clip(lower=0)\n",
    "    s = np.log1p(s)\n",
    "    hi = s.quantile(0.99)\n",
    "    s = s.clip(upper=hi)\n",
    "    mean = s.mean()\n",
    "    std = s.std() or 1.0\n",
    "    df_final_sample_transformed[c] = (s - mean) / std\n",
    "\n",
    "# leave as is:\n",
    "# tsli_user_streamer_missing, tslc_user_streamer_missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841013c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_8 = [\n",
    "    # clip_0,1\n",
    "    \"ctr_user_streamer_7d\",\n",
    "    \"ctr_user_category_7d\",\n",
    "\n",
    "    # log1p + clip p99 + standardize\n",
    "    \"num_click_user_streamer_7d\",\n",
    "    \"num_imp_user_streamer_7d\",\n",
    "    \"num_click_user_category_7d\",\n",
    "    \"num_imp_user_category_7d\",\n",
    "    \"time_since_last_impression_user_streamer\",\n",
    "    \"time_since_last_click_user_streamer\",\n",
    "]\n",
    "\n",
    "# for col in features_8:\n",
    "#     explore_numeric_feature(df_final_sample_transformed, col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0e51da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "out_path = Path(\"..\") / \"data\" / \"draft_sample.csv\"\n",
    "df_final_sample_transformed.to_csv(out_path, index=False)\n",
    "print(f\"Wrote: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a84cb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32927/3241656936.py:4: DtypeWarning: Columns (11,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, parse_dates=[\"imp_timestamp\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 352307\n",
      "imp_timestamp min: 2025-05-04 14:10:59.857000\n",
      "imp_timestamp max: 2025-05-25 14:59:47.745000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = \"/workspace/multi_stage_ranking_kuaishou/data/draft_sample.csv\"\n",
    "df = pd.read_csv(path, parse_dates=[\"imp_timestamp\"])\n",
    "\n",
    "print(\"rows:\", len(df))\n",
    "print(\"imp_timestamp min:\", df[\"imp_timestamp\"].min())\n",
    "print(\"imp_timestamp max:\", df[\"imp_timestamp\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bf83f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
